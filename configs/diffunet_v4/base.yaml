# =========================
# Experiment meta
# =========================
experiment:
  name: DiffUnet
  version: 5.1
  description: "Add Body Filled Mask Channel in the input"
  tags: ["DiffUNet", "colon"]
  debug: false
  hash: null

seed: null

logging:
  use_aim: true                  # will be used only if utils exist
  # aim_repo: "/home/yb107/cvpr2025/DukeDiffSeg/aim/"
  aim_repo: "/home/yb107/cvpr2025/aim_repo/dukediffseg/aim/"

# =========================
# Data
# ========================= 
data:
  description: "Mixed Mobina Colon Data (full 96^3 volumes)"
  num_classes: 13                 # background + 12 organs (original label space)
  condition_labels: [3,4,5,6,7,8,9,10,11,12]  # colon + small bowel
  use_spacing_maps: false
  spacing_channels: 3             # (dx, dy, dz)
  orientation: "RAS"
  pixdim: [1.5, 1.5, 2.0]
  roi_size: [96, 96, 96]
  # roi_size: [128, 128, 128]
  slice_axis: 2
  body_filled_channel: true

  # cache_dir: "/data/usr/yb107/colon_data/cache_mobina_mixed_colon_dataset"

  # cache_dir: "/data/usr/yb107/colon_data/cache_mobina_mixed_colon_dataset_test"
  cache_dir: "/data/usr/yb107/colon_data/cache_mobina_mixed_colon_dataset_body_filled_tmp"

  # cache_dir: "/data/usr/yb107/colon_data/cache_mobina_mixed_colon_dataset_binary"
  # cache_dir: "/data/usr/yb107/colon_data/cache_mobina_mixed_colon_dataset_liver"
  cache_n_transforms: 'all' # 9 or all

  # train_jsonl: "/home/yb107/cvpr2025/DukeDiffSeg/data/mobina_mixed_colon_dataset/mobina_mixed_colon_train.jsonl"
  # val_jsonl:   "/home/yb107/cvpr2025/DukeDiffSeg/data/mobina_mixed_colon_dataset/mobina_mixed_colon_test.jsonl"
  train_jsonl: "/home/yb107/cvpr2025/DukeDiffSeg/data/mobina_mixed_colon_dataset/mobina_mixed_colon_dataset_with_body_filled_train.jsonl"
  val_jsonl:  "/home/yb107/cvpr2025/DukeDiffSeg/data/mobina_mixed_colon_dataset/mobina_mixed_colon_dataset_with_body_filled_val.jsonl"

  batch_size_per_gpu: 1
  num_workers_per_gpu: 4
  val_batch_size: 1
  val_num_workers: 4
  shuffle_train_data: true
  save_data: false

# ───────────────────────────────────────────────────────────
# Model architecture (easy to swap out a different 'model.params' block)
model:
  name: "DiffUNet"
  
  adverserial_train:
    enabled: false
    start_epoch: 100
    adverserial_loss_weight: 0.01
  
  pca_likelihood:
    enabled: false
    pca_model_path: "/home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-colon/4.2/colon_pca_cov_eig.pt"

  params:
    spatial_dims: 3
    in_channels:  2 # Overridden 
    out_channels: 1 # Overridden 
    features: [32, 64, 128, 256, 512, 64]
    activation: ["LeakyReLU", {"negative_slope": 0.1, "inplace": False}]
    beta_schedule: "linear"
    use_spacing_info: false  # Use spacing information in the model

    dropout: 0.0
    use_checkpointing: true

# ───────────────────────────────────────────────────────────
# Diffusion-specific hyperparameters
diffusion:
  diffusion_steps:   1000
  ddim_steps:      10
  beta_schedule:    "linear" # Options: "linear", "cosine", 
  model_mean_type:  "start_x" # Options: "eps", "start_x"
  # model_mean_type:  "eps" # Options: "eps", "start_x"
  guidance_scale: 1.0      # Scale for classifier-free guidance
  condition_drop_prob: 0.3  # Probability of dropping the condition for classifier-free guidance

  schedule_sampler:
    name: "uniform"
    max_steps: 1000
 
  clip_denoised:     true
  ddim:              false


# task: "liver"
task: "colon"

constraint: "binary"
  

# =========================
# Training runtime
# =========================
training:
  inference_mode: false
  
  epochs: 2000
  device: "cuda"
  save_dir: "/home/yb107/cvpr2025/DukeDiffSeg/outputs"
  save_interval: 5
  start_epoch: null
  num_gpus: 1
  save_config_yaml: true
  accumulate_grad_steps: 1

  resume: 
    # path: /home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-colon/4.16/checkpoints/training/DiffUnet-binary-colon_training_best_checkpoint_580_MeanDice0.4582.pt
    path: null
    restore_optimizer: true
    restore_scheduler: true
    restore_ema: true
    restore_discriminator: false


optimizer:
  name:       "AdamW"
  # lr: 1e-3
  # weight_decay: 1e-4
  lr:         2e-4
  weight_decay: 1e-3
  set_to_none: true  # Set optimizer parameters to None for memory efficiency


# =========================
# Exponential moving average (EMA)
# =========================
ema:
  enable: true
  rate: 0.9999


# =========================
# LR Scheduler (applied to Stage-2 trainer)
# =========================
lr_scheduler:
  name: "LinearWarmupCosineAnnealingLR"
  warmup_epochs: 50
  max_epochs: 300

# =========================
# Evaluation & visualization
# =========================
evaluation:
  validation_interval: 20
  validation_max_num_samples: 40
  metrics: ["Mean Dice"]
  early_stopping:
    enabled: false
    patience: 20
  visualize: true
  visualize_every_iter: 2

  save_outputs:
    enabled: false                # dump inputs/outputs during validation
    dir: ${training.save_dir}/inference
    dir_postfix: null
    save_inputs: false             # also save ground truth label
    save_config_yaml: true
    pred_postfix: "pred"
    label_postfix: "label"
    input_postfix: "input"

# =========================
# AMP
# =========================
amp:
  enabled: false
  fp16_scale_growth: 1e-3
