{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1255a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MINIMAL INFERENCE SETUP FOR JUPYTER NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import monai\n",
    "from monai import transforms\n",
    "from monai.networks.nets import DiffusionModelUNet\n",
    "from monai.networks.schedulers import DDIMScheduler, DDPMScheduler\n",
    "\n",
    "# Import your custom utilities (adjust paths as needed)\n",
    "from utils.data import MaskToSDFd, sdf_to_mask\n",
    "from utils.monai_transforms import (\n",
    "    HarmonizeLabelsd,\n",
    "    AddSpacingTensord,\n",
    "    FilterAndRelabeld,\n",
    "    EnsureAllTorchd,\n",
    "    CropForegroundAxisd,\n",
    ")\n",
    "\n",
    "from monai.transforms import Transform\n",
    "\n",
    "\n",
    "class ProbeTransform(Transform):\n",
    "    def __init__(self, message=\"ProbeTransform called\"):\n",
    "        super().__init__()\n",
    "        self.message = message\n",
    "\n",
    "    def __call__(self, data):\n",
    "        print(self.message)\n",
    "        return data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ============================================================================\n",
    "class VolumeSpacingEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(4, 64),  # 4 = volume (1) + spacing (3)\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, volume, spacing):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            volume: (B,) or (B, 1) - organ volume in ml\n",
    "            spacing: (B, 3) - [spacing_x, spacing_y, spacing_z] in mm\n",
    "        Returns:\n",
    "            embedding: (B, embed_dim)\n",
    "        \"\"\"\n",
    "        if volume.dim() == 1:\n",
    "            volume = volume.unsqueeze(-1)  # (B, 1)\n",
    "        # Concatenate volume and spacing\n",
    "        volume_spacing = torch.cat([volume, spacing], dim=-1)  # (B, 4)\n",
    "        return self.mlp(volume_spacing)\n",
    "\n",
    "\n",
    "class InferenceConfig:\n",
    "    # Model params\n",
    "    spatial_dims = 3\n",
    "    in_channels = 1  # image SDF + conditioning\n",
    "    out_channels = 1  # target organ SDF\n",
    "    features = [32, 64, 64, 128, 256]  # adjust based on your trained model\n",
    "    attention_levels = [False, False, False, False, False]\n",
    "    num_head_channels = [0, 0, 0, 64, 64]\n",
    "    with_conditioning = True\n",
    "    cross_attention_dim = 128  # adjust based on your trained model\n",
    "    volume_embedding_dim = 128\n",
    "\n",
    "    # Diffusion params\n",
    "    diffusion_steps = 1000\n",
    "    ddim_steps = 20\n",
    "    beta_schedule = \"scaled_linear_beta\"\n",
    "    model_mean_type = \"sample\"  # or \"sample\"\n",
    "    guidance_scale = 1.0  # CFG scale\n",
    "    condition_drop_prob = 0.1\n",
    "\n",
    "    # Data params\n",
    "    pixdim = (1.5, 1.5, 2.0)\n",
    "    orientation = \"RAS\"\n",
    "    roi_size = (128, 128, 128)\n",
    "\n",
    "    # Paths\n",
    "    checkpoint_path = None\n",
    "    # checkpoint_path = \"/home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-iterative/7.2/DiffUnet-binary-iterative_liver_latest_checkpoint_97.pt\"\n",
    "    device = \"cuda:1\"\n",
    "\n",
    "\n",
    "config = InferenceConfig()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ORGAN MAPPING (from your script)\n",
    "# ============================================================================\n",
    "\n",
    "ORGAN_NAMES = {\n",
    "    1: \"colon\",\n",
    "    2: \"rectum\",\n",
    "    3: \"small_bowel\",\n",
    "    4: \"stomach\",\n",
    "    5: \"liver\",\n",
    "    6: \"spleen\",\n",
    "    7: \"kidneys\",\n",
    "    9: \"pancreas\",\n",
    "    10: \"urinary_bladder\",\n",
    "    11: \"duodenum\",\n",
    "    12: \"gallbladder\",\n",
    "}\n",
    "NAME_TO_INDEX = {v: k for k, v in ORGAN_NAMES.items()}\n",
    "\n",
    "\n",
    "def get_conditioning_organs(generation_order, target_organ_index):\n",
    "    \"\"\"Get list of organs to condition on\"\"\"\n",
    "    if target_organ_index not in generation_order:\n",
    "        raise ValueError(f\"Target organ {target_organ_index} not in order\")\n",
    "    pos = generation_order.index(target_organ_index)\n",
    "    return generation_order[:pos]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. BUILD PREPROCESSING TRANSFORM\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def build_inference_transform(config, target_organ=\"liver\", generation_order=None):\n",
    "    \"\"\"Simplified transform for single-sample inference\"\"\"\n",
    "\n",
    "    target_organ_index = NAME_TO_INDEX.get(target_organ)\n",
    "    if generation_order is None:\n",
    "        generation_order = [5, 6, 7, 9, 3, 1, 2, 4, 10, 11, 12]  # default order\n",
    "\n",
    "    conditioning_organs = get_conditioning_organs(generation_order, target_organ_index)\n",
    "\n",
    "    data_keys = [\"image\", \"label\", \"body_filled_channel\"]\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=data_keys),\n",
    "            transforms.EnsureChannelFirstd(keys=data_keys),\n",
    "            transforms.Spacingd(keys=data_keys, pixdim=config.pixdim, mode=\"nearest\"),\n",
    "            transforms.Orientationd(keys=data_keys, axcodes=config.orientation),\n",
    "            ProbeTransform(message=\"üêî After Orientationd\"),\n",
    "            # transforms.KeepLargestConnectedComponentd(keys=data_keys),\n",
    "            # ProbeTransform(message=\"üê∏ After KeepLargestConnectedComponentd\"),\n",
    "            HarmonizeLabelsd(keys=[\"image\", \"label\"], kidneys_same_index=True),\n",
    "            CropForegroundAxisd(\n",
    "                keys=data_keys,\n",
    "                source_key=\"image\",\n",
    "                axis=2,\n",
    "                margin=5,\n",
    "            ),\n",
    "            transforms.CropForegroundd(\n",
    "                keys=data_keys, source_key=\"body_filled_channel\", margin=5\n",
    "            ),\n",
    "            ProbeTransform(message=\"üê¢ After CropForegroundd\"),\n",
    "            transforms.Resized(\n",
    "                keys=data_keys, spatial_size=config.roi_size, mode=\"nearest\"\n",
    "            ),\n",
    "            AddSpacingTensord(ref_key=\"image\"),\n",
    "            FilterAndRelabeld(\n",
    "                image_key=\"image\",\n",
    "                label_key=\"label\",\n",
    "                conditioning_organs=conditioning_organs,\n",
    "                target_organ=target_organ_index,\n",
    "            ),\n",
    "            ProbeTransform(message=\"üêç After FilterAndRelabeld\"),\n",
    "            MaskToSDFd(\n",
    "                keys=data_keys,\n",
    "                spacing_key=\"spacing_tensor\",\n",
    "                device=torch.device(\"cpu\"),\n",
    "            ),\n",
    "            ProbeTransform(message=\"üêô After MaskToSDFd\"),\n",
    "            EnsureAllTorchd(print_changes=False),\n",
    "            transforms.EnsureTyped(\n",
    "                keys=data_keys + [\"spacing_tensor\"],\n",
    "                track_meta=True,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transform\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. BUILD MODEL\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    \"\"\"Create and load model\"\"\"\n",
    "\n",
    "    model = DiffusionModelUNet(\n",
    "        spatial_dims=config.spatial_dims,\n",
    "        in_channels=config.in_channels\n",
    "        + config.out_channels,  # concat image during inference\n",
    "        out_channels=config.out_channels,\n",
    "        channels=config.features,\n",
    "        attention_levels=config.attention_levels,\n",
    "        num_res_blocks=1,\n",
    "        transformer_num_layers=0,\n",
    "        num_head_channels=config.num_head_channels,\n",
    "        with_conditioning=config.with_conditioning,\n",
    "        cross_attention_dim=config.cross_attention_dim,\n",
    "    )\n",
    "\n",
    "    vol_embed = VolumeSpacingEmbedding(embed_dim=config.volume_embedding_dim)\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(config.checkpoint_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    vol_embed.load_state_dict(checkpoint[\"vol_embed_net\"])\n",
    "\n",
    "    model = model.to(config.device)\n",
    "    vol_embed = vol_embed.to(config.device)\n",
    "    model.eval()\n",
    "    vol_embed.eval()\n",
    "\n",
    "    print(f\"‚úì Model loaded from {config.checkpoint_path}\")\n",
    "    print(f\"‚úì Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "\n",
    "    return model, vol_embed\n",
    "\n",
    "\n",
    "def build_scheduler(config):\n",
    "    \"\"\"Create DDIM scheduler for inference\"\"\"\n",
    "\n",
    "    scheduler = DDIMScheduler(\n",
    "        num_train_timesteps=config.diffusion_steps,\n",
    "        beta_start=0.0001,\n",
    "        beta_end=0.02,\n",
    "        schedule=config.beta_schedule,\n",
    "        clip_sample=False,\n",
    "        prediction_type=config.model_mean_type,\n",
    "    )\n",
    "    scheduler.set_timesteps(num_inference_steps=config.ddim_steps)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(\n",
    "    model, vol_embed, scheduler, image_sdf, body_filled_sdf, volume, spacing, config\n",
    "):\n",
    "    \"\"\"\n",
    "    Run DDIM sampling to generate organ mask\n",
    "\n",
    "    Args:\n",
    "        model: DiffusionModelUNet\n",
    "        scheduler: DDIMScheduler\n",
    "        image_sdf: conditioning image SDF [B, 1, H, W, D]\n",
    "        config: InferenceConfig\n",
    "\n",
    "    Returns:\n",
    "        pred_mask: binary mask [B, 1, H, W, D]\n",
    "        pred_sdf: signed distance field [B, 1, H, W, D]\n",
    "    \"\"\"\n",
    "\n",
    "    device = config.device\n",
    "    image_sdf = image_sdf.to(device).float()\n",
    "    pred = torch.randn_like(image_sdf)\n",
    "\n",
    "    # Initialize with random noise\n",
    "    if body_filled_sdf is not None:\n",
    "        body_filled_sdf = body_filled_sdf.to(device).float()\n",
    "        image = torch.cat([image_sdf, body_filled_sdf], dim=1)\n",
    "    else:\n",
    "        image = image_sdf\n",
    "\n",
    "    vol_context = vol_embed(volume.to(device), spacing.to(device))\n",
    "\n",
    "    # Get all timesteps\n",
    "    all_next_timesteps = torch.cat(\n",
    "        [scheduler.timesteps[1:], torch.tensor([0], dtype=scheduler.timesteps.dtype)]\n",
    "    )\n",
    "\n",
    "    # DDIM sampling loop\n",
    "    for i, (t, next_t) in enumerate(zip(scheduler.timesteps, all_next_timesteps)):\n",
    "        pred_before = pred.clone()\n",
    "        # Concatenate conditioning\n",
    "        model_input = torch.cat([pred, image], dim=1)\n",
    "\n",
    "        # Predict\n",
    "        # t_tensor = torch.full((image.shape[0],), t, device=device).long()\n",
    "        # model_output = model(x=model_input, timesteps=t_tensor, context=vol_context)\n",
    "        model_output = model(\n",
    "            x=model_input,\n",
    "            timesteps=torch.Tensor((t,)).to(image.device),\n",
    "            context=vol_embed,\n",
    "        )\n",
    "\n",
    "        # Classifier-free guidance (if scale != 1.0)\n",
    "        if config.guidance_scale != 1.0:\n",
    "            image_uncond = torch.zeros_like(image)\n",
    "            model_input_uncond = torch.cat([pred, image_uncond], dim=1)\n",
    "            uncond_output = model(\n",
    "                x=model_input_uncond,\n",
    "                timesteps=torch.Tensor((t,)).to(image.device),\n",
    "                context=vol_embed,\n",
    "            )\n",
    "            model_output = uncond_output + config.guidance_scale * (\n",
    "                model_output - uncond_output\n",
    "            )\n",
    "\n",
    "        # DDIM step\n",
    "        pred, _ = scheduler.step(model_output, t, pred)\n",
    "\n",
    "        change = (pred - pred_before).abs().mean().item()\n",
    "        print(f\"Step {i}: t={t}, change={change:.6f}, pred_mean={pred.mean():.4f}\")\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Step {i+1}/{len(scheduler.timesteps)}\")\n",
    "\n",
    "    # Convert SDF to mask\n",
    "    pred_sdf = pred.clone()\n",
    "    pred_mask = sdf_to_mask(pred * 10.0)  # scale factor from your training\n",
    "\n",
    "    return pred_mask, pred_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba50b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jsonl_path = \"/home/yb107/cvpr2025/DukeDiffSeg/data/mobina_mixed_colon_dataset/mobina_mixed_colon_dataset_with_body_filled_test.jsonl\"\n",
    "import json\n",
    "\n",
    "\n",
    "def load_jsonl_inference(jsonl_path):\n",
    "    data = []\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "test_data = load_jsonl_inference(test_jsonl_path)\n",
    "test_data = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3260aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Preprocessing data...\n",
      "üèãÔ∏è‚Äç‚ôÄÔ∏è Applying transforms...\n",
      "üêî After Orientationd\n",
      "üê¢ After CropForegroundd\n",
      "üêç After FilterAndRelabeld\n",
      "üêô After MaskToSDFd\n"
     ]
    }
   ],
   "source": [
    "# 1. Preprocess\n",
    "config = InferenceConfig()\n",
    "# config.checkpoint_path = \"/home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-iterative/7.1/checkpoints/final_unet.pth\"\n",
    "config.checkpoint_path = \"/home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-iterative/7.1/checkpoints/liver/DiffUnet-binary-iterative_liver_latest_checkpoint_2000.pt\"\n",
    "config.in_channels = 2\n",
    "\n",
    "print(\"üì¶ Preprocessing data...\")\n",
    "transform = build_inference_transform(\n",
    "    config, \"liver\", [5, 12, 6, 7, 4, 9, 11, 10, 2, 1, 3]\n",
    ")\n",
    "\n",
    "data_dict = {\n",
    "    \"image\": test_data[\"mask\"],\n",
    "    \"label\": test_data[\"mask\"],\n",
    "    \"body_filled_channel\": test_data[\"body_filled_mask\"],\n",
    "}\n",
    "print(\"üèãÔ∏è‚Äç‚ôÄÔ∏è Applying transforms...\")\n",
    "data_dict = transform(data_dict)\n",
    "\n",
    "# SAVE data_dict as .pt for caching\n",
    "# torch.save(data_dict, \"tmp/data_dict.pt\")\n",
    "\n",
    "# load data_dict from .pt\n",
    "data_dict = torch.load(\"tmp/data_dict.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f59ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  Building model...\n",
      "‚úì Model loaded from /home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-iterative/7.1/checkpoints/liver/DiffUnet-binary-iterative_liver_latest_checkpoint_2000.pt\n",
      "‚úì Parameters: 28.98M\n"
     ]
    }
   ],
   "source": [
    "# 2. Build model & scheduler\n",
    "print(\"üèóÔ∏è  Building model...\")\n",
    "model, vol_net = build_model(config)\n",
    "scheduler = build_scheduler(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5c5feb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metatensor([1685.4402])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_sdf = data_dict[\"image\"].unsqueeze(0)  # [1, 1, H, W, D]\n",
    "label_sdf = data_dict[\"label\"].unsqueeze(0)\n",
    "label_mask = sdf_to_mask(label_sdf)\n",
    "body_filled_sdf = data_dict[\"body_filled_channel\"].unsqueeze(0)\n",
    "zero_filled_sdf = torch.zeros_like(body_filled_sdf)\n",
    "spacing = data_dict[\"spacing_tensor\"].unsqueeze(0)\n",
    "\n",
    "voxel_volume = spacing.prod(dim=1)\n",
    "label_volume = label_mask.sum(dim=(1, 2, 3, 4)) * voxel_volume\n",
    "label_volume / 1000.0  # convert to ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7890adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Running DDIM sampling...\n",
      "Step 0: t=950, change=0.009661, pred_mean=-0.0089\n",
      "Step 1: t=900, change=0.013250, pred_mean=-0.0210\n",
      "Step 2: t=850, change=0.017306, pred_mean=-0.0369\n",
      "Step 3: t=800, change=0.021589, pred_mean=-0.0567\n",
      "Step 4: t=750, change=0.026396, pred_mean=-0.0802\n",
      "  Step 5/20\n",
      "Step 5: t=700, change=0.031898, pred_mean=-0.1071\n",
      "Step 6: t=650, change=0.038110, pred_mean=-0.1368\n",
      "Step 7: t=600, change=0.044929, pred_mean=-0.1683\n",
      "Step 8: t=550, change=0.051992, pred_mean=-0.2004\n",
      "Step 9: t=500, change=0.058836, pred_mean=-0.2321\n",
      "  Step 10/20\n",
      "Step 10: t=450, change=0.064867, pred_mean=-0.2623\n",
      "Step 11: t=400, change=0.069573, pred_mean=-0.2898\n",
      "Step 12: t=350, change=0.072536, pred_mean=-0.3142\n",
      "Step 13: t=300, change=0.073518, pred_mean=-0.3349\n",
      "Step 14: t=250, change=0.072468, pred_mean=-0.3518\n",
      "  Step 15/20\n",
      "Step 15: t=200, change=0.069479, pred_mean=-0.3651\n",
      "Step 16: t=150, change=0.064911, pred_mean=-0.3752\n",
      "Step 17: t=100, change=0.059776, pred_mean=-0.3828\n",
      "Step 18: t=50, change=0.069310, pred_mean=-0.3903\n",
      "Step 19: t=0, change=0.009733, pred_mean=-0.3930\n",
      "  Step 20/20\n"
     ]
    }
   ],
   "source": [
    "print(\"üé® Running DDIM sampling...\")\n",
    "pred_mask, pred_sdf = run_inference(\n",
    "    model,\n",
    "    vol_net,\n",
    "    scheduler,\n",
    "    image_sdf,\n",
    "    zero_filled_sdf,\n",
    "    label_volume + 5.4e3,\n",
    "    spacing,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64d64f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metatensor([1619.9487])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate pred_mask original volume\n",
    "pred_mask_volume = pred_mask.sum(dim=(1, 2, 3, 4)).cpu() * voxel_volume\n",
    "pred_mask_volume / 1000.0  # convert to ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b51858ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-07 14:12:53,489 INFO image_writer.py:197 - writing: tmp/Patient_00074_Study_78614_Series_03__pred_71_smaller.nii.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "metatensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monai.transforms.SaveImage(\n",
    "    output_dir=\"tmp/\",\n",
    "    output_postfix=\"_pred_71_smaller\",\n",
    "    separate_folder=False,\n",
    ")(pred_mask.squeeze(0))\n",
    "# monai.transforms.SaveImage(\n",
    "#     output_dir=\"tmp/\",\n",
    "#     output_postfix=\"_pred_sdf\",\n",
    "#     separate_folder=False,\n",
    "# )(pred_sdf.squeeze(0))\n",
    "# monai.transforms.SaveImage(\n",
    "#     output_dir=\"tmp/\",\n",
    "#     output_postfix=\"_img_sdf\",\n",
    "#     separate_folder=False,\n",
    "# )(label_sdf.squeeze(0))\n",
    "# monai.transforms.SaveImage(\n",
    "#     output_dir=\"tmp/\",\n",
    "#     output_postfix=\"_body_filled_sdf\",\n",
    "#     separate_folder=False,\n",
    "# )(body_filled_sdf.squeeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DukeDiffSeg-HooVw7aP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
