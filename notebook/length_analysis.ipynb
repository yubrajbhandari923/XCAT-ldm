{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46b6038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "from monai import transforms\n",
    "import os\n",
    "import functools\n",
    "from monai.utils.enums import CommonKeys as Keys\n",
    "from utils.data import (\n",
    "    add_spacing,\n",
    "    binary_mask_labels,\n",
    "    remove_labels,\n",
    "    transform_labels,\n",
    "    list_from_jsonl,\n",
    "    dataset_depended_transform_labels,\n",
    "    mask_to_sdf,\n",
    "    sdf_to_mask,\n",
    "    MaskToSDFd,\n",
    ")\n",
    "from utils.monai_transforms import CropForegroundAxisd, SmoothColonMaskd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1fdb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(train=True):\n",
    "    \"\"\"Get the MONAI transforms for training or validation.\"\"\"\n",
    "\n",
    "    def custom_name_formatter(meta_dict, saver):\n",
    "        full_path = meta_dict[\"filename_or_obj\"]\n",
    "        base = os.path.basename(full_path)\n",
    "        # If the filename itself contains \"colon\", pull the parent folder as the ID\n",
    "\n",
    "        if \"labels\" in full_path.lower():\n",
    "            postfix = \"_label\"\n",
    "        else:\n",
    "            postfix = \"_image\"\n",
    "\n",
    "        return {\"filename\": f\"{base.replace('.nii.gz', '')}\"}\n",
    "\n",
    "    data_keys = [Keys.IMAGE, Keys.LABEL]\n",
    "\n",
    "    custom_transforms = [\n",
    "        transforms.LoadImaged(keys=data_keys),\n",
    "        transforms.EnsureChannelFirstd(keys=data_keys),\n",
    "        transforms.Spacingd(\n",
    "            keys=data_keys,\n",
    "            pixdim=[1.5, 1.5, 2.0],\n",
    "            mode=\"nearest\",\n",
    "        ),\n",
    "        transforms.Orientationd(\n",
    "            keys=data_keys,\n",
    "            axcodes=\"RAS\",\n",
    "        ),\n",
    "        transforms.KeepLargestConnectedComponentd(\n",
    "            keys=[Keys.LABEL, Keys.IMAGE],\n",
    "        ),\n",
    "        transforms.Lambdad(\n",
    "            keys=[Keys.IMAGE, Keys.LABEL],\n",
    "            func=functools.partial(\n",
    "                dataset_depended_transform_labels,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    return transforms.Compose(custom_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54cd9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training data...\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00101_Study_73554_Series_04.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_02190_Study_01823_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01769_Study_52102_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00248_Study_02847_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_02065_Study_50544_Series_05.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_02227_Study_46685_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00128_Study_43204_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00829_Study_41138_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00407_Study_70735_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00250_Study_18800_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00883_Study_12220_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00072_Study_11534_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00511_Study_27382_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00003_Study_47072_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_02080_Study_81271_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00305_Study_86454_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00910_Study_40711_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00192_Study_04711_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00426_Study_24513_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00113_Study_84581_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00434_Study_27654_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01383_Study_76258_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00842_Study_27715_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01097_Study_33411_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00413_Study_64665_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01769_Study_17040_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00931_Study_06807_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00040_Study_23517_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00689_Study_03517_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01684_Study_58370_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01099_Study_46868_Series_04.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00201_Study_03053_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00239_Study_51288_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00047_Study_00223_Series_04.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_02087_Study_20258_Series_102.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00297_Study_16620_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00896_Study_63568_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00432_Study_05870_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00837_Study_60638_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01772_Study_64185_Series_05.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00395_Study_30685_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00692_Study_38864_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00296_Study_51182_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00913_Study_53327_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00766_Study_52631_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00611_Study_28077_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00900_Study_36075_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01772_Study_52761_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00374_Study_07323_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00074_Study_78614_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00669_Study_70052_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00749_Study_83750_Series_05.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00577_Study_40617_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00749_Study_28625_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00437_Study_10216_Series_02.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01555_Study_24545_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00359_Study_72742_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00391_Study_46455_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00330_Study_82268_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00370_Study_26660_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00245_Study_73333_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01650_Study_13674_Series_04.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01852_Study_13382_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00719_Study_54481_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00286_Study_33712_Series_05.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01930_Study_41043_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00047_Study_00223_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00864_Study_50424_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00361_Study_03238_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01068_Study_07187_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00484_Study_77283_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00065_Study_50878_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01470_Study_10344_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00417_Study_10780_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00679_Study_03061_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01230_Study_48701_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00421_Study_35353_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01852_Study_06088_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00629_Study_06810_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00150_Study_24804_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01475_Study_03586_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00571_Study_86356_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00287_Study_77204_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01295_Study_48667_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00731_Study_34146_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00174_Study_83786_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01321_Study_65244_Series_04.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01184_Study_17516_Series_05.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00101_Study_04807_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01220_Study_67105_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00796_Study_72446_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00542_Study_03811_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01174_Study_07134_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01046_Study_08153_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00570_Study_53842_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00518_Study_52425_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00454_Study_21284_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00759_Study_83876_Series_101.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01434_Study_41158_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00478_Study_40688_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00323_Study_46638_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_02297_Study_61216_Series_05.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00695_Study_67447_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01275_Study_86112_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01883_Study_25873_Series_05.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00145_Study_36210_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00509_Study_57068_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01194_Study_63084_Series_05.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_02106_Study_56306_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00263_Study_27588_Series_04.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00375_Study_76567_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00161_Study_20011_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00749_Study_46167_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00212_Study_14571_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01741_Study_28302_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01214_Study_12436_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01691_Study_58425_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00841_Study_08652_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00873_Study_01053_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00589_Study_31555_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00779_Study_34435_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00865_Study_83183_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00774_Study_60515_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01992_Study_16161_Series_05.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00700_Study_02874_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00604_Study_71410_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00067_Study_14786_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01330_Study_36314_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00607_Study_31843_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_00146_Study_38237_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01860_Study_00700_Series_05.nii.gz: name 'skeletonize_3d' is not defined\n",
      "Error processing /data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/masks/Patient_01792_Study_30860_Series_03.nii.gz: name 'skeletonize_3d' is not defined\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 388\u001b[0m\n\u001b[1;32m    383\u001b[0m     summary \u001b[38;5;241m=\u001b[39m create_comparison_plots(df_all, output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplots\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalysis complete! Check the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplots\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory for visualizations.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 388\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 367\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    364\u001b[0m spacing \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m1.5\u001b[39m, \u001b[38;5;241m2.0\u001b[39m]  \u001b[38;5;66;03m# From your transforms\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 367\u001b[0m df_training \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_training_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_jsonl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing C-grade data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    370\u001b[0m df_c_grade \u001b[38;5;241m=\u001b[39m process_c_grade_data(c_grade_jsonl, pred_dir, spacing)\n",
      "Cell \u001b[0;32mIn[11], line 148\u001b[0m, in \u001b[0;36mprocess_training_data\u001b[0;34m(jsonl_path, spacing)\u001b[0m\n\u001b[1;32m    145\u001b[0m patient_id \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(mask_path)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.nii.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Measure colon (assuming label 1 is colon after dataset_depended_transform_labels)\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m length_mm, volume_ml \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_measure_colon\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspacing\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Colon label\u001b[39;49;00m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length_mm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    154\u001b[0m         {\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: patient_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    161\u001b[0m         }\n\u001b[1;32m    162\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[11], line 117\u001b[0m, in \u001b[0;36mload_and_measure_colon\u001b[0;34m(file_path, label_value, spacing)\u001b[0m\n\u001b[1;32m    114\u001b[0m     volume_ml \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(colon_mask) \u001b[38;5;241m*\u001b[39m voxel_volume_mm3 \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000.0\u001b[39m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Calculate length\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     length_mm \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_colon_length_centerline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolon_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspacing\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m length_mm, volume_ml\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[11], line 59\u001b[0m, in \u001b[0;36mcompute_colon_length_centerline\u001b[0;34m(mask_array, spacing)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_colon_length_centerline\u001b[39m(mask_array, spacing):\n\u001b[1;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    More sophisticated centerline-based length calculation.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m    Uses distance transform and skeleton to trace the colon path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m        length_mm: Colon length in millimeters\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     binary_mask \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mmask_array\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# Compute skeleton\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     skeleton \u001b[38;5;241m=\u001b[39m skeletonize_3d(binary_mask)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy import ndimage\n",
    "from skimage.morphology import skeletonize\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def compute_colon_length_from_skeleton(mask_array, spacing):\n",
    "    \"\"\"\n",
    "    Compute colon length using skeletonization method.\n",
    "\n",
    "    Args:\n",
    "        mask_array: Binary mask of colon (numpy array)\n",
    "        spacing: Voxel spacing in mm [x, y, z]\n",
    "\n",
    "    Returns:\n",
    "        length_mm: Estimated colon length in millimeters\n",
    "    \"\"\"\n",
    "    # Ensure binary mask\n",
    "    binary_mask = (mask_array > 0).astype(np.uint8)\n",
    "\n",
    "    # Skeletonize the colon\n",
    "    skeleton = skeletonize(binary_mask)\n",
    "\n",
    "    # Get skeleton coordinates\n",
    "    coords = np.argwhere(skeleton > 0)\n",
    "\n",
    "    if len(coords) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate length by summing distances between consecutive skeleton points\n",
    "    # This is an approximation; more sophisticated methods trace the actual path\n",
    "    length_voxels = np.sum(skeleton)\n",
    "\n",
    "    # Convert to physical distance using spacing\n",
    "    # Average spacing for isotropic approximation\n",
    "    avg_spacing = np.mean(spacing)\n",
    "    length_mm = length_voxels * avg_spacing\n",
    "\n",
    "    return length_mm\n",
    "\n",
    "\n",
    "def compute_colon_length_centerline(mask_array, spacing):\n",
    "    \"\"\"\n",
    "    More sophisticated centerline-based length calculation.\n",
    "    Uses distance transform and skeleton to trace the colon path.\n",
    "\n",
    "    Args:\n",
    "        mask_array: Binary mask of colon\n",
    "        spacing: Voxel spacing [x, y, z] in mm\n",
    "\n",
    "    Returns:\n",
    "        length_mm: Colon length in millimeters\n",
    "    \"\"\"\n",
    "    binary_mask = (mask_array > 0).astype(np.uint8)\n",
    "\n",
    "    # Compute skeleton\n",
    "    skeleton = skeletonize(binary_mask)\n",
    "    skeleton_coords = np.argwhere(skeleton > 0)\n",
    "\n",
    "    if len(skeleton_coords) < 2:\n",
    "        return 0.0\n",
    "\n",
    "    # Build a graph of skeleton points and find the longest path\n",
    "    # For simplicity, we'll sum edge lengths in the skeleton\n",
    "    # Scale by spacing to get physical distance\n",
    "\n",
    "    total_length = 0.0\n",
    "\n",
    "    # For each skeleton voxel, check 26-connected neighbors\n",
    "    for i in range(len(skeleton_coords) - 1):\n",
    "        coord1 = skeleton_coords[i]\n",
    "        coord2 = skeleton_coords[i + 1]\n",
    "\n",
    "        # Calculate Euclidean distance in physical space\n",
    "        diff = (coord1 - coord2) * spacing\n",
    "        distance = np.linalg.norm(diff)\n",
    "        total_length += distance\n",
    "\n",
    "    return total_length\n",
    "\n",
    "\n",
    "def load_and_measure_colon(file_path, label_value=None, spacing=[1.5, 1.5, 2.0]):\n",
    "    \"\"\"\n",
    "    Load a NIfTI file and measure colon length.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to .nii.gz file\n",
    "        label_value: If multi-label, specify colon label (1 for your case).\n",
    "                     If None, treats as binary mask\n",
    "        spacing: Physical spacing after preprocessing\n",
    "\n",
    "    Returns:\n",
    "        length_mm: Colon length in millimeters\n",
    "        volume_ml: Colon volume in milliliters\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load NIfTI\n",
    "        nii = nib.load(file_path)\n",
    "        data = nii.get_fdata()\n",
    "\n",
    "        # Extract colon mask\n",
    "        if label_value is not None:\n",
    "            colon_mask = data == label_value\n",
    "        else:\n",
    "            colon_mask = data > 0\n",
    "\n",
    "        # Calculate volume\n",
    "        voxel_volume_mm3 = np.prod(spacing)\n",
    "        volume_ml = np.sum(colon_mask) * voxel_volume_mm3 / 1000.0\n",
    "\n",
    "        # Calculate length\n",
    "        length_mm = compute_colon_length_centerline(colon_mask, spacing)\n",
    "\n",
    "        return length_mm, volume_ml\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "def process_training_data(jsonl_path, spacing=[1.5, 1.5, 2.0]):\n",
    "    \"\"\"\n",
    "    Process training data and extract colon measurements.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path: Path to training data JSONL file\n",
    "        spacing: Physical spacing\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with measurements\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            mask_path = data[\"mask\"]\n",
    "\n",
    "            # Extract patient ID\n",
    "            patient_id = os.path.basename(mask_path).replace(\".nii.gz\", \"\")\n",
    "\n",
    "            # Measure colon (assuming label 1 is colon after dataset_depended_transform_labels)\n",
    "            length_mm, volume_ml = load_and_measure_colon(\n",
    "                mask_path, label_value=1, spacing=spacing  # Colon label\n",
    "            )\n",
    "\n",
    "            if length_mm is not None:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"patient_id\": patient_id,\n",
    "                        \"dataset\": \"training_a_grade\",\n",
    "                        \"length_mm\": length_mm,\n",
    "                        \"length_cm\": length_mm / 10.0,\n",
    "                        \"volume_ml\": volume_ml,\n",
    "                        \"file_path\": mask_path,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def process_c_grade_data(jsonl_path, pred_dir=None, spacing=[1.5, 1.5, 2.0]):\n",
    "    \"\"\"\n",
    "    Process C-grade data (original masks and predictions if available).\n",
    "\n",
    "    Args:\n",
    "        jsonl_path: Path to C-grade JSONL file\n",
    "        pred_dir: Directory containing _pred.nii.gz files\n",
    "        spacing: Physical spacing\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with measurements\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            mask_path = data[\"mask\"]\n",
    "\n",
    "            # Extract patient ID\n",
    "            patient_id = os.path.basename(mask_path).replace(\".nii.gz\", \"\")\n",
    "\n",
    "            # Measure original C-grade mask\n",
    "            length_mm, volume_ml = load_and_measure_colon(\n",
    "                mask_path, label_value=2, spacing=spacing\n",
    "            )\n",
    "\n",
    "            if length_mm is not None:\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"patient_id\": patient_id,\n",
    "                        \"dataset\": \"c_grade_original\",\n",
    "                        \"length_mm\": length_mm,\n",
    "                        \"length_cm\": length_mm / 10.0,\n",
    "                        \"volume_ml\": volume_ml,\n",
    "                        \"file_path\": mask_path,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # Measure predicted mask if available\n",
    "            if pred_dir is not None:\n",
    "                pred_path = os.path.join(pred_dir, f\"{patient_id}_pred.nii.gz\")\n",
    "                if os.path.exists(pred_path):\n",
    "                    length_mm_pred, volume_ml_pred = load_and_measure_colon(\n",
    "                        pred_path,\n",
    "                        label_value=None,  # Binary prediction\n",
    "                        spacing=spacing,\n",
    "                    )\n",
    "\n",
    "                    if length_mm_pred is not None:\n",
    "                        results.append(\n",
    "                            {\n",
    "                                \"patient_id\": patient_id,\n",
    "                                \"dataset\": \"c_grade_predicted\",\n",
    "                                \"length_mm\": length_mm_pred,\n",
    "                                \"length_cm\": length_mm_pred / 10.0,\n",
    "                                \"volume_ml\": volume_ml_pred,\n",
    "                                \"file_path\": pred_path,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def create_comparison_plots(df, output_dir=\"plots\"):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization plots for colon length distributions.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with measurements\n",
    "        output_dir: Directory to save plots\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Plot 1: Distribution comparison (box plot)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    datasets = df[\"dataset\"].unique()\n",
    "    data_to_plot = [df[df[\"dataset\"] == d][\"length_cm\"].dropna() for d in datasets]\n",
    "\n",
    "    plt.boxplot(data_to_plot, labels=datasets)\n",
    "    plt.ylabel(\"Colon Length (cm)\", fontsize=12)\n",
    "    plt.title(\n",
    "        \"Colon Length Distribution Across Datasets\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    plt.xticks(rotation=15, ha=\"right\")\n",
    "    plt.grid(axis=\"y\", alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"length_distribution_boxplot.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Histogram comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for dataset in datasets:\n",
    "        subset = df[df[\"dataset\"] == dataset][\"length_cm\"].dropna()\n",
    "        plt.hist(subset, bins=20, alpha=0.5, label=dataset, edgecolor=\"black\")\n",
    "\n",
    "    plt.xlabel(\"Colon Length (cm)\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.title(\"Colon Length Distribution Histogram\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, \"length_distribution_histogram.png\"), dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 3: Paired comparison for C-grade (original vs predicted)\n",
    "    c_grade_orig = df[df[\"dataset\"] == \"c_grade_original\"].set_index(\"patient_id\")\n",
    "    c_grade_pred = df[df[\"dataset\"] == \"c_grade_predicted\"].set_index(\"patient_id\")\n",
    "\n",
    "    # Find common patients\n",
    "    common_patients = c_grade_orig.index.intersection(c_grade_pred.index)\n",
    "\n",
    "    if len(common_patients) > 0:\n",
    "        plt.figure(figsize=(10, 10))\n",
    "\n",
    "        orig_lengths = c_grade_orig.loc[common_patients, \"length_cm\"]\n",
    "        pred_lengths = c_grade_pred.loc[common_patients, \"length_cm\"]\n",
    "\n",
    "        plt.scatter(orig_lengths, pred_lengths, alpha=0.6, s=100)\n",
    "\n",
    "        # Add diagonal line (perfect agreement)\n",
    "        min_val = min(orig_lengths.min(), pred_lengths.min())\n",
    "        max_val = max(orig_lengths.max(), pred_lengths.max())\n",
    "        plt.plot(\n",
    "            [min_val, max_val],\n",
    "            [min_val, max_val],\n",
    "            \"r--\",\n",
    "            linewidth=2,\n",
    "            label=\"Perfect Agreement\",\n",
    "        )\n",
    "\n",
    "        plt.xlabel(\"Original C-Grade Length (cm)\", fontsize=12)\n",
    "        plt.ylabel(\"Predicted Length (cm)\", fontsize=12)\n",
    "        plt.title(\n",
    "            \"C-Grade: Original vs Predicted Colon Length\",\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.axis(\"equal\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"c_grade_comparison_scatter.png\"), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # Plot 4: Improvement plot (difference)\n",
    "        differences = pred_lengths - orig_lengths\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.bar(\n",
    "            range(len(differences)),\n",
    "            differences.values,\n",
    "            color=[\"green\" if d > 0 else \"red\" for d in differences.values],\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "        plt.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "        plt.xlabel(\"Patient Index\", fontsize=12)\n",
    "        plt.ylabel(\"Length Difference (Predicted - Original) cm\", fontsize=12)\n",
    "        plt.title(\n",
    "            \"Colon Length Change: Predicted vs Original C-Grade\",\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        plt.grid(axis=\"y\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, \"length_improvement.png\"), dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # Generate summary statistics\n",
    "    summary = (\n",
    "        df.groupby(\"dataset\")[\"length_cm\"]\n",
    "        .agg([\"count\", \"mean\", \"std\", \"min\", \"max\", \"median\"])\n",
    "        .round(2)\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COLON LENGTH SUMMARY STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(summary)\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "    # Save summary to CSV\n",
    "    summary.to_csv(os.path.join(output_dir, \"length_summary_statistics.csv\"))\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function.\n",
    "    \"\"\"\n",
    "    # Define paths - UPDATE THESE TO YOUR ACTUAL PATHS\n",
    "    training_jsonl = \"/home/yb107/cvpr2025/DukeDiffSeg/data/mobina_mixed_colon_dataset/mobina_mixed_colon_dataset_with_body_filled.jsonl\"\n",
    "    c_grade_jsonl = \"/home/yb107/cvpr2025/DukeDiffSeg/data/c_grade_colons/3d_vlsmv2_c_grade_colon_dataset_with_body_filled.jsonl\"\n",
    "    pred_dir = \"/home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-colon/5.1/inference_c_grade_550_gs_2.0_final_small_with_skeletonization\"\n",
    "\n",
    "    spacing = [1.5, 1.5, 2.0]  # From your transforms\n",
    "\n",
    "    print(\"Processing training data...\")\n",
    "    df_training = process_training_data(training_jsonl, spacing)\n",
    "\n",
    "    print(\"Processing C-grade data...\")\n",
    "    df_c_grade = process_c_grade_data(c_grade_jsonl, pred_dir, spacing)\n",
    "\n",
    "    # Combine all results\n",
    "    df_all = pd.concat([df_training, df_c_grade], ignore_index=True)\n",
    "\n",
    "    # Save raw data\n",
    "    df_all.to_csv(\"colon_length_measurements.csv\", index=False)\n",
    "    print(\n",
    "        f\"\\nSaved measurements for {len(df_all)} cases to colon_length_measurements.csv\"\n",
    "    )\n",
    "\n",
    "    # Create plots\n",
    "    print(\"\\nGenerating visualization plots...\")\n",
    "    summary = create_comparison_plots(df_all, output_dir=\"plots\")\n",
    "\n",
    "    print(\"\\nAnalysis complete! Check the 'plots' directory for visualizations.\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17e48726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -onai (/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from seaborn) (2.3.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from seaborn) (3.10.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Ignoring invalid distribution -onai (/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb81c78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COLON LENGTH ANALYSIS USING MONAI TRANSFORMS\n",
      "================================================================================\n",
      "\n",
      "[1/5] Loading data dictionaries...\n",
      "  Loaded 419 training cases\n",
      "  Loaded 67 C-grade original cases\n",
      "  Loaded 11 C-grade prediction cases\n",
      "\n",
      "[2/5] Processing datasets with MONAI transforms...\n",
      "\n",
      "  Processing training data...\n",
      "Processing 419 cases...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/nibabel/loadsave.py\", line 101, in load\n    stat_result = os.stat(filename)\nFileNotFoundError: [Errno 2] No such file or directory: '/data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/images/Patient_00101_Study_73554_Series_04.nii.gz'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 150, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/io/dictionary.py\", line 163, in __call__\n    data = self._loader(d[key], reader)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/io/array.py\", line 263, in __call__\n    img = reader.read(filename)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/data/image_reader.py\", line 1088, in read\n    img = nib.load(name, **kwargs_)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/nibabel/loadsave.py\", line 103, in load\n    raise FileNotFoundError(f\"No such file or no access: '{filename}'\")\nFileNotFoundError: No such file or no access: '/data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/images/Patient_00101_Study_73554_Series_04.nii.gz'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 150, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/compose.py\", line 346, in __call__\n    result = execute_compose(\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/compose.py\", line 116, in execute_compose\n    data = apply_transform(\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 180, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x7fd5366ce290>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/data/dataset.py\", line 108, in __getitem__\n    return self._transform(index)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/data/dataset.py\", line 94, in _transform\n    return self.transform(data_i)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/compose.py\", line 346, in __call__\n    result = execute_compose(\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/compose.py\", line 116, in execute_compose\n    data = apply_transform(\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 180, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.compose.Compose object at 0x7fd4d969d540>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 993\u001b[0m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll results saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mOUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m directory\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 993\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 947\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[2/5] Processing datasets with MONAI transforms...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    946\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Processing training data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 947\u001b[0m df_training \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_dataset_with_monai\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dicts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_WORKERS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_binary_pred\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Processing C-grade original data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    955\u001b[0m df_c_grade \u001b[38;5;241m=\u001b[39m process_dataset_with_monai(\n\u001b[1;32m    956\u001b[0m     c_grade_dicts,\n\u001b[1;32m    957\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m    958\u001b[0m     num_workers\u001b[38;5;241m=\u001b[39mNUM_WORKERS,\n\u001b[1;32m    959\u001b[0m     is_binary_pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    960\u001b[0m )\n",
      "Cell \u001b[0;32mIn[15], line 486\u001b[0m, in \u001b[0;36mprocess_dataset_with_monai\u001b[0;34m(data_dicts, batch_size, num_workers, is_binary_pred)\u001b[0m\n\u001b[1;32m    483\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data_dicts)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cases...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 486\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m batch_data:\n\u001b[1;32m    488\u001b[0m         result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatient_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    490\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_diameter_mm\u001b[39m\u001b[38;5;124m\"\u001b[39m: item[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolon_avg_diameter_mm\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    497\u001b[0m         }\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1480\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1505\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1505\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/torch/_utils.py:733\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 733\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/nibabel/loadsave.py\", line 101, in load\n    stat_result = os.stat(filename)\nFileNotFoundError: [Errno 2] No such file or directory: '/data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/images/Patient_00101_Study_73554_Series_04.nii.gz'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 150, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/io/dictionary.py\", line 163, in __call__\n    data = self._loader(d[key], reader)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/io/array.py\", line 263, in __call__\n    img = reader.read(filename)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/data/image_reader.py\", line 1088, in read\n    img = nib.load(name, **kwargs_)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/nibabel/loadsave.py\", line 103, in load\n    raise FileNotFoundError(f\"No such file or no access: '{filename}'\")\nFileNotFoundError: No such file or no access: '/data/usr/yb107/colon_data/refined_by_mobina/a_grade_colons_not_in_refined_by_md/images/Patient_00101_Study_73554_Series_04.nii.gz'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 150, in apply_transform\n    return _apply_transform(transform, data, unpack_items, lazy, overrides, log_stats)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 98, in _apply_transform\n    return transform(data, lazy=lazy) if isinstance(transform, LazyTrait) else transform(data)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/compose.py\", line 346, in __call__\n    result = execute_compose(\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/compose.py\", line 116, in execute_compose\n    data = apply_transform(\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 180, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.io.dictionary.LoadImaged object at 0x7fd5366ce290>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/data/dataset.py\", line 108, in __getitem__\n    return self._transform(index)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/data/dataset.py\", line 94, in _transform\n    return self.transform(data_i)\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/compose.py\", line 346, in __call__\n    result = execute_compose(\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/compose.py\", line 116, in execute_compose\n    data = apply_transform(\n  File \"/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/transforms/transform.py\", line 180, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.compose.Compose object at 0x7fd4d969d540>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import functools\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from monai import transforms\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.transforms import MapTransform\n",
    "from scipy import ndimage\n",
    "from skimage.morphology import skeletonize\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "\n",
    "# Your existing transform setup\n",
    "class Keys:\n",
    "    IMAGE = \"image\"\n",
    "    LABEL = \"label\"\n",
    "    MASK = \"mask\"\n",
    "    BODY_FILLED_MASK = \"body_filled_mask\"\n",
    "\n",
    "\n",
    "def dataset_depended_transform_labels(data, label_mapping=None):\n",
    "    \"\"\"\n",
    "    Your existing label transformation function.\n",
    "    Maps labels to standardized organ labels:\n",
    "      1: colon\n",
    "      2: rectum\n",
    "      3: small_bowel\n",
    "      etc.\n",
    "    \"\"\"\n",
    "    # Placeholder - implement your actual transform logic here\n",
    "    # This should handle the label mapping from your multi-organ segmentation\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_data_dicts_from_jsonl(\n",
    "    jsonl_path: str, dataset_type: str = \"training\"\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Load data from JSONL and create MONAI-compatible data dictionaries.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path: Path to JSONL file\n",
    "        dataset_type: \"training\", \"c_grade_original\", or \"c_grade_predicted\"\n",
    "\n",
    "    Returns:\n",
    "        List of data dictionaries\n",
    "    \"\"\"\n",
    "    data_dicts = []\n",
    "\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "\n",
    "            if dataset_type == \"training\":\n",
    "                # Training data has mask and body_filled_mask\n",
    "                # Need to infer image path from mask path\n",
    "                mask_path = data[\"mask\"]\n",
    "                image_path = mask_path.replace(\"/masks/\", \"/images/\").replace(\n",
    "                    \"_mask\", \"\"\n",
    "                )\n",
    "\n",
    "                data_dict = {\n",
    "                    Keys.IMAGE: image_path,\n",
    "                    Keys.LABEL: mask_path,\n",
    "                    Keys.MASK: mask_path,\n",
    "                    Keys.BODY_FILLED_MASK: data[\"body_filled_mask\"],\n",
    "                    \"dataset_type\": dataset_type,\n",
    "                    \"patient_id\": os.path.basename(mask_path).replace(\".nii.gz\", \"\"),\n",
    "                }\n",
    "\n",
    "            elif dataset_type == \"c_grade_original\":\n",
    "                # C-grade data has image, mask, and body_filled_mask\n",
    "                data_dict = {\n",
    "                    Keys.IMAGE: data[\"image\"],\n",
    "                    Keys.LABEL: data[\"mask\"],\n",
    "                    Keys.MASK: data[\"mask\"],\n",
    "                    Keys.BODY_FILLED_MASK: data[\"body_filled_mask\"],\n",
    "                    \"dataset_type\": dataset_type,\n",
    "                    \"patient_id\": os.path.basename(data[\"mask\"]).replace(\".nii.gz\", \"\"),\n",
    "                }\n",
    "\n",
    "            data_dicts.append(data_dict)\n",
    "\n",
    "    return data_dicts\n",
    "\n",
    "\n",
    "def get_c_grade_prediction_dicts(jsonl_path: str, pred_dir: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Create data dicts for C-grade predictions.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path: Path to original C-grade JSONL\n",
    "        pred_dir: Directory containing _pred.nii.gz files\n",
    "\n",
    "    Returns:\n",
    "        List of data dictionaries with predictions\n",
    "    \"\"\"\n",
    "    data_dicts = []\n",
    "\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            patient_id = os.path.basename(data[\"mask\"]).replace(\".nii.gz\", \"\")\n",
    "            pred_path = os.path.join(pred_dir, f\"{patient_id}_pred.nii.gz\")\n",
    "\n",
    "            if os.path.exists(pred_path):\n",
    "                # For predictions, we use the pred as the label (binary mask)\n",
    "                data_dict = {\n",
    "                    Keys.IMAGE: data[\"image\"],\n",
    "                    Keys.LABEL: pred_path,  # Use prediction as label\n",
    "                    Keys.MASK: pred_path,\n",
    "                    \"dataset_type\": \"c_grade_predicted\",\n",
    "                    \"patient_id\": patient_id,\n",
    "                    \"is_binary_pred\": True,  # Flag to skip label transformation\n",
    "                }\n",
    "                data_dicts.append(data_dict)\n",
    "\n",
    "    return data_dicts\n",
    "\n",
    "\n",
    "def get_transforms_for_analysis(is_binary_pred=False):\n",
    "    \"\"\"\n",
    "    Get MONAI transforms for analysis (modified from your get_transforms).\n",
    "\n",
    "    Args:\n",
    "        is_binary_pred: If True, skip label transformation for binary predictions\n",
    "\n",
    "    Returns:\n",
    "        Composed transforms\n",
    "    \"\"\"\n",
    "    data_keys = [Keys.IMAGE, Keys.LABEL]\n",
    "\n",
    "    custom_transforms = [\n",
    "        transforms.LoadImaged(keys=data_keys, image_only=False),\n",
    "        transforms.EnsureChannelFirstd(keys=data_keys),\n",
    "        transforms.Spacingd(\n",
    "            keys=data_keys,\n",
    "            pixdim=[1.5, 1.5, 2.0],\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        transforms.Orientationd(\n",
    "            keys=data_keys,\n",
    "            axcodes=\"RAS\",\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    # Only apply label transformation and connected component for non-binary predictions\n",
    "    if not is_binary_pred:\n",
    "        custom_transforms.extend(\n",
    "            [\n",
    "                transforms.KeepLargestConnectedComponentd(\n",
    "                    keys=[Keys.LABEL],\n",
    "                    applied_labels=[1],  # Only for colon label\n",
    "                ),\n",
    "                transforms.Lambdad(\n",
    "                    keys=[Keys.LABEL],\n",
    "                    func=functools.partial(dataset_depended_transform_labels),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        # For binary predictions, just ensure it's binary\n",
    "        custom_transforms.append(\n",
    "            transforms.Lambdad(\n",
    "                keys=[Keys.LABEL],\n",
    "                func=lambda x: (x > 0).astype(np.float32),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return transforms.Compose(custom_transforms)\n",
    "\n",
    "\n",
    "class ColonLengthMetricsd(MapTransform):\n",
    "    \"\"\"\n",
    "    MONAI transform to compute colon length and volume metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: List[str],\n",
    "        colon_label: int = 1,\n",
    "        spacing: List[float] = [1.5, 1.5, 2.0],\n",
    "        is_binary: bool = False,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            keys: Keys to compute metrics for (typically [Keys.LABEL])\n",
    "            colon_label: Label value for colon (1 by default)\n",
    "            spacing: Physical spacing in mm\n",
    "            is_binary: If True, treat as binary mask instead of multi-label\n",
    "            allow_missing_keys: Whether to allow missing keys\n",
    "        \"\"\"\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.colon_label = colon_label\n",
    "        self.spacing = np.array(spacing)\n",
    "        self.is_binary = is_binary\n",
    "\n",
    "    def _compute_skeleton_length(self, binary_mask: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute colon length using 3D skeletonization.\n",
    "\n",
    "        Args:\n",
    "            binary_mask: Binary colon mask\n",
    "\n",
    "        Returns:\n",
    "            Length in millimeters\n",
    "        \"\"\"\n",
    "        if binary_mask.sum() == 0:\n",
    "            return 0.0\n",
    "\n",
    "        # Ensure 3D array (remove channel dimension if present)\n",
    "        if binary_mask.ndim == 4:\n",
    "            binary_mask = binary_mask[0]\n",
    "\n",
    "        # Skeletonize\n",
    "        skeleton = skeletonize(binary_mask.astype(np.uint8), method=\"lee\")\n",
    "\n",
    "        # Get skeleton coordinates\n",
    "        skeleton_points = np.argwhere(skeleton > 0)\n",
    "\n",
    "        if len(skeleton_points) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        # Method 1: Simple skeleton voxel count scaled by average spacing\n",
    "        # This is fast but less accurate\n",
    "        # length_mm = skeleton.sum() * np.mean(self.spacing)\n",
    "\n",
    "        # Method 2: Sum of distances between consecutive points\n",
    "        # Sort points to create a path (simplified - not optimal path)\n",
    "        # For better results, you'd use minimum spanning tree or graph algorithms\n",
    "\n",
    "        # Calculate pairwise distances in physical space\n",
    "        skeleton_points_physical = skeleton_points * self.spacing\n",
    "\n",
    "        # Use minimum spanning tree approach for better path estimation\n",
    "        length_mm = self._compute_path_length_mst(skeleton_points_physical)\n",
    "\n",
    "        return length_mm\n",
    "\n",
    "    def _compute_path_length_mst(self, points: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute path length using Minimum Spanning Tree approach.\n",
    "        This provides a better approximation of the actual colon path.\n",
    "\n",
    "        Args:\n",
    "            points: Skeleton points in physical coordinates (N x 3)\n",
    "\n",
    "        Returns:\n",
    "            Total path length in mm\n",
    "        \"\"\"\n",
    "        if len(points) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        # For very long paths, sample points to avoid memory issues\n",
    "        max_points = 5000\n",
    "        if len(points) > max_points:\n",
    "            indices = np.random.choice(len(points), max_points, replace=False)\n",
    "            points = points[indices]\n",
    "\n",
    "        # Compute pairwise distances\n",
    "        try:\n",
    "            distances = pdist(points, metric=\"euclidean\")\n",
    "            dist_matrix = squareform(distances)\n",
    "\n",
    "            # Simple MST using Prim's algorithm\n",
    "            n_points = len(points)\n",
    "            visited = np.zeros(n_points, dtype=bool)\n",
    "            visited[0] = True\n",
    "            total_length = 0.0\n",
    "\n",
    "            for _ in range(n_points - 1):\n",
    "                min_dist = np.inf\n",
    "                min_idx = -1\n",
    "\n",
    "                for i in range(n_points):\n",
    "                    if visited[i]:\n",
    "                        for j in range(n_points):\n",
    "                            if not visited[j] and dist_matrix[i, j] < min_dist:\n",
    "                                min_dist = dist_matrix[i, j]\n",
    "                                min_idx = j\n",
    "\n",
    "                if min_idx != -1:\n",
    "                    visited[min_idx] = True\n",
    "                    total_length += min_dist\n",
    "\n",
    "            return total_length\n",
    "\n",
    "        except MemoryError:\n",
    "            # Fallback: simple consecutive point distances\n",
    "            sorted_indices = np.lexsort((points[:, 2], points[:, 1], points[:, 0]))\n",
    "            sorted_points = points[sorted_indices]\n",
    "            diffs = np.diff(sorted_points, axis=0)\n",
    "            distances = np.linalg.norm(diffs, axis=1)\n",
    "            return np.sum(distances)\n",
    "\n",
    "    def _compute_centerline_length(self, binary_mask: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Alternative method: Compute length using distance transform and centerline.\n",
    "        More accurate for tubular structures.\n",
    "\n",
    "        Args:\n",
    "            binary_mask: Binary colon mask\n",
    "\n",
    "        Returns:\n",
    "            Length in millimeters\n",
    "        \"\"\"\n",
    "        if binary_mask.sum() == 0:\n",
    "            return 0.0\n",
    "\n",
    "        if binary_mask.ndim == 4:\n",
    "            binary_mask = binary_mask[0]\n",
    "\n",
    "        # Compute distance transform\n",
    "        distance = ndimage.distance_transform_edt(binary_mask, sampling=self.spacing)\n",
    "\n",
    "        # Find centerline by skeletonizing the mask\n",
    "        skeleton = skeletonize(binary_mask.astype(np.uint8), method=\"lee\")\n",
    "\n",
    "        # Get skeleton coordinates\n",
    "        skeleton_coords = np.argwhere(skeleton > 0)\n",
    "\n",
    "        if len(skeleton_coords) < 2:\n",
    "            return 0.0\n",
    "\n",
    "        # Convert to physical coordinates\n",
    "        skeleton_physical = skeleton_coords * self.spacing\n",
    "\n",
    "        # Compute path length\n",
    "        length_mm = self._compute_path_length_mst(skeleton_physical)\n",
    "\n",
    "        return length_mm\n",
    "\n",
    "    def _compute_volume(self, binary_mask: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute colon volume in milliliters.\n",
    "\n",
    "        Args:\n",
    "            binary_mask: Binary colon mask\n",
    "\n",
    "        Returns:\n",
    "            Volume in milliliters\n",
    "        \"\"\"\n",
    "        if binary_mask.ndim == 4:\n",
    "            binary_mask = binary_mask[0]\n",
    "\n",
    "        voxel_volume_mm3 = np.prod(self.spacing)\n",
    "        volume_ml = binary_mask.sum() * voxel_volume_mm3 / 1000.0\n",
    "\n",
    "        return float(volume_ml)\n",
    "\n",
    "    def _compute_surface_area(self, binary_mask: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute approximate surface area using voxel counting.\n",
    "\n",
    "        Args:\n",
    "            binary_mask: Binary colon mask\n",
    "\n",
    "        Returns:\n",
    "            Surface area in cm\n",
    "        \"\"\"\n",
    "        if binary_mask.ndim == 4:\n",
    "            binary_mask = binary_mask[0]\n",
    "\n",
    "        # Count boundary voxels (simple approach)\n",
    "        # More sophisticated: use marching cubes for actual surface\n",
    "        eroded = ndimage.binary_erosion(binary_mask)\n",
    "        boundary = binary_mask & ~eroded\n",
    "\n",
    "        # Approximate surface area\n",
    "        voxel_face_area = np.mean(\n",
    "            [\n",
    "                self.spacing[i] * self.spacing[j]\n",
    "                for i in range(3)\n",
    "                for j in range(i + 1, 3)\n",
    "            ]\n",
    "        )\n",
    "        surface_area_cm2 = boundary.sum() * voxel_face_area / 100.0\n",
    "\n",
    "        return float(surface_area_cm2)\n",
    "\n",
    "    def __call__(self, data: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Apply the transform to compute metrics.\n",
    "\n",
    "        Args:\n",
    "            data: Data dictionary\n",
    "\n",
    "        Returns:\n",
    "            Data dictionary with added metrics\n",
    "        \"\"\"\n",
    "        d = dict(data)\n",
    "\n",
    "        for key in self.key_iterator(d):\n",
    "            label_array = d[key]\n",
    "\n",
    "            # Convert to numpy if tensor\n",
    "            if torch.is_tensor(label_array):\n",
    "                label_array = label_array.numpy()\n",
    "\n",
    "            # Extract colon mask\n",
    "            if self.is_binary:\n",
    "                colon_mask = (label_array > 0).astype(np.uint8)\n",
    "            else:\n",
    "                colon_mask = (label_array == self.colon_label).astype(np.uint8)\n",
    "\n",
    "            # Compute metrics\n",
    "            length_skeleton = self._compute_skeleton_length(colon_mask)\n",
    "            length_centerline = self._compute_centerline_length(colon_mask)\n",
    "            volume = self._compute_volume(colon_mask)\n",
    "            surface_area = self._compute_surface_area(colon_mask)\n",
    "\n",
    "            # Add metrics to data dictionary\n",
    "            d[\"colon_length_skeleton_mm\"] = length_skeleton\n",
    "            d[\"colon_length_centerline_mm\"] = length_centerline\n",
    "            d[\"colon_length_cm\"] = length_centerline / 10.0  # Use centerline as primary\n",
    "            d[\"colon_volume_ml\"] = volume\n",
    "            d[\"colon_surface_area_cm2\"] = surface_area\n",
    "\n",
    "            # Additional derived metrics\n",
    "            if volume > 0 and length_centerline > 0:\n",
    "                d[\"colon_avg_diameter_mm\"] = np.sqrt(\n",
    "                    volume * 1000 / (np.pi * length_centerline)\n",
    "                )\n",
    "            else:\n",
    "                d[\"colon_avg_diameter_mm\"] = 0.0\n",
    "\n",
    "        return d\n",
    "\n",
    "\n",
    "def process_dataset_with_monai(\n",
    "    data_dicts: List[Dict],\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 4,\n",
    "    is_binary_pred: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process dataset using MONAI pipeline to compute colon metrics.\n",
    "\n",
    "    Args:\n",
    "        data_dicts: List of data dictionaries\n",
    "        batch_size: Batch size for processing\n",
    "        num_workers: Number of workers for data loading\n",
    "        is_binary_pred: Whether this is binary prediction data\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with all computed metrics\n",
    "    \"\"\"\n",
    "    # Get transforms\n",
    "    base_transforms = get_transforms_for_analysis(is_binary_pred=is_binary_pred)\n",
    "\n",
    "    # Add metric computation transform\n",
    "    metric_transform = ColonLengthMetricsd(\n",
    "        keys=[Keys.LABEL],\n",
    "        colon_label=1,\n",
    "        spacing=[1.5, 1.5, 2.0],\n",
    "        is_binary=is_binary_pred,\n",
    "    )\n",
    "\n",
    "    # Combine transforms\n",
    "    all_transforms = transforms.Compose([base_transforms, metric_transform])\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = Dataset(data=data_dicts, transform=all_transforms)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=list,  # Return list of dicts instead of batched dict\n",
    "    )\n",
    "\n",
    "    # Process all data\n",
    "    results = []\n",
    "\n",
    "    print(f\"Processing {len(data_dicts)} cases...\")\n",
    "    for batch_idx, batch_data in enumerate(dataloader):\n",
    "        for item in batch_data:\n",
    "            result = {\n",
    "                \"patient_id\": item[\"patient_id\"],\n",
    "                \"dataset_type\": item[\"dataset_type\"],\n",
    "                \"length_skeleton_mm\": item[\"colon_length_skeleton_mm\"],\n",
    "                \"length_centerline_mm\": item[\"colon_length_centerline_mm\"],\n",
    "                \"length_cm\": item[\"colon_length_cm\"],\n",
    "                \"volume_ml\": item[\"colon_volume_ml\"],\n",
    "                \"surface_area_cm2\": item[\"colon_surface_area_cm2\"],\n",
    "                \"avg_diameter_mm\": item[\"colon_avg_diameter_mm\"],\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {(batch_idx + 1) * batch_size}/{len(data_dicts)} cases\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def create_comprehensive_plots(df: pd.DataFrame, output_dir: str = \"plots\"):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization plots.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with all measurements\n",
    "        output_dir: Output directory for plots\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Set style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"husl\")\n",
    "\n",
    "    # Plot 1: Length distribution comparison (box + violin)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Box plot\n",
    "    df.boxplot(column=\"length_cm\", by=\"dataset_type\", ax=axes[0])\n",
    "    axes[0].set_title(\n",
    "        \"Colon Length Distribution (Box Plot)\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[0].set_xlabel(\"Dataset\", fontsize=12)\n",
    "    axes[0].set_ylabel(\"Colon Length (cm)\", fontsize=12)\n",
    "    axes[0].get_figure().suptitle(\"\")  # Remove automatic title\n",
    "    plt.setp(axes[0].xaxis.get_majorticklabels(), rotation=15, ha=\"right\")\n",
    "\n",
    "    # Violin plot\n",
    "    sns.violinplot(data=df, x=\"dataset_type\", y=\"length_cm\", ax=axes[1])\n",
    "    axes[1].set_title(\n",
    "        \"Colon Length Distribution (Violin Plot)\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Dataset\", fontsize=12)\n",
    "    axes[1].set_ylabel(\"Colon Length (cm)\", fontsize=12)\n",
    "    plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=15, ha=\"right\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, \"01_length_distribution.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 2: Histogram overlays\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for dataset in df[\"dataset_type\"].unique():\n",
    "        subset = df[df[\"dataset_type\"] == dataset][\"length_cm\"].dropna()\n",
    "        plt.hist(\n",
    "            subset, bins=30, alpha=0.5, label=dataset, edgecolor=\"black\", linewidth=0.5\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Colon Length (cm)\", fontsize=12)\n",
    "    plt.ylabel(\"Frequency\", fontsize=12)\n",
    "    plt.title(\n",
    "        \"Colon Length Distribution - Histogram Overlay\", fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(output_dir, \"02_length_histogram.png\"),\n",
    "        dpi=300,\n",
    "        bbox_inches=\"tight\",\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    # Plot 3: Paired comparison for C-grade (original vs predicted)\n",
    "    c_orig = df[df[\"dataset_type\"] == \"c_grade_original\"].set_index(\"patient_id\")\n",
    "    c_pred = df[df[\"dataset_type\"] == \"c_grade_predicted\"].set_index(\"patient_id\")\n",
    "\n",
    "    common_patients = c_orig.index.intersection(c_pred.index)\n",
    "\n",
    "    if len(common_patients) > 0:\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "        orig_lengths = c_orig.loc[common_patients, \"length_cm\"].values\n",
    "        pred_lengths = c_pred.loc[common_patients, \"length_cm\"].values\n",
    "\n",
    "        # Scatter plot\n",
    "        axes[0].scatter(\n",
    "            orig_lengths,\n",
    "            pred_lengths,\n",
    "            alpha=0.6,\n",
    "            s=100,\n",
    "            edgecolors=\"black\",\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "\n",
    "        # Perfect agreement line\n",
    "        min_val = min(orig_lengths.min(), pred_lengths.min())\n",
    "        max_val = max(orig_lengths.max(), pred_lengths.max())\n",
    "        axes[0].plot(\n",
    "            [min_val, max_val],\n",
    "            [min_val, max_val],\n",
    "            \"r--\",\n",
    "            linewidth=2,\n",
    "            label=\"Perfect Agreement\",\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Add correlation coefficient\n",
    "        correlation = np.corrcoef(orig_lengths, pred_lengths)[0, 1]\n",
    "        axes[0].text(\n",
    "            0.05,\n",
    "            0.95,\n",
    "            f\"r = {correlation:.3f}\",\n",
    "            transform=axes[0].transAxes,\n",
    "            fontsize=12,\n",
    "            verticalalignment=\"top\",\n",
    "            bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.5),\n",
    "        )\n",
    "\n",
    "        axes[0].set_xlabel(\"Original C-Grade Length (cm)\", fontsize=12)\n",
    "        axes[0].set_ylabel(\"Predicted Length (cm)\", fontsize=12)\n",
    "        axes[0].set_title(\n",
    "            \"Original vs Predicted: Correlation\", fontsize=14, fontweight=\"bold\"\n",
    "        )\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "        axes[0].set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "        # Bland-Altman plot\n",
    "        mean_lengths = (orig_lengths + pred_lengths) / 2\n",
    "        diff_lengths = pred_lengths - orig_lengths\n",
    "        mean_diff = np.mean(diff_lengths)\n",
    "        std_diff = np.std(diff_lengths)\n",
    "\n",
    "        axes[1].scatter(\n",
    "            mean_lengths,\n",
    "            diff_lengths,\n",
    "            alpha=0.6,\n",
    "            s=100,\n",
    "            edgecolors=\"black\",\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "        axes[1].axhline(\n",
    "            mean_diff,\n",
    "            color=\"blue\",\n",
    "            linestyle=\"-\",\n",
    "            linewidth=2,\n",
    "            label=f\"Mean Diff: {mean_diff:.2f} cm\",\n",
    "        )\n",
    "        axes[1].axhline(\n",
    "            mean_diff + 1.96 * std_diff,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"+1.96 SD: {mean_diff + 1.96*std_diff:.2f} cm\",\n",
    "        )\n",
    "        axes[1].axhline(\n",
    "            mean_diff - 1.96 * std_diff,\n",
    "            color=\"red\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"-1.96 SD: {mean_diff - 1.96*std_diff:.2f} cm\",\n",
    "        )\n",
    "        axes[1].axhline(0, color=\"black\", linestyle=\"-\", linewidth=1, alpha=0.3)\n",
    "\n",
    "        axes[1].set_xlabel(\"Mean Length (cm)\", fontsize=12)\n",
    "        axes[1].set_ylabel(\"Difference (Predicted - Original) cm\", fontsize=12)\n",
    "        axes[1].set_title(\"Bland-Altman Plot\", fontsize=14, fontweight=\"bold\")\n",
    "        axes[1].legend(fontsize=9)\n",
    "        axes[1].grid(alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(output_dir, \"03_c_grade_comparison.png\"),\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        # Plot 4: Individual patient improvement bars\n",
    "        differences = pred_lengths - orig_lengths\n",
    "\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        colors = [\"green\" if d > 0 else \"red\" for d in differences]\n",
    "        bars = plt.bar(\n",
    "            range(len(differences)),\n",
    "            differences,\n",
    "            color=colors,\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"black\",\n",
    "            linewidth=0.5,\n",
    "        )\n",
    "\n",
    "        plt.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=1.5)\n",
    "        plt.axhline(\n",
    "            y=mean_diff,\n",
    "            color=\"blue\",\n",
    "            linestyle=\"--\",\n",
    "            linewidth=2,\n",
    "            label=f\"Mean: {mean_diff:.2f} cm\",\n",
    "        )\n",
    "\n",
    "        plt.xlabel(\"Patient Index\", fontsize=12)\n",
    "        plt.ylabel(\"Length Difference (Predicted - Original) cm\", fontsize=12)\n",
    "        plt.title(\n",
    "            \"Per-Patient Length Change: Improvement Analysis\",\n",
    "            fontsize=14,\n",
    "            fontweight=\"bold\",\n",
    "        )\n",
    "        plt.legend()\n",
    "        plt.grid(axis=\"y\", alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(output_dir, \"04_improvement_bars.png\"),\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "        # Plot 5: Volume vs Length relationship\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "        for dataset in df[\"dataset_type\"].unique():\n",
    "            subset = df[df[\"dataset_type\"] == dataset]\n",
    "            axes[0].scatter(\n",
    "                subset[\"length_cm\"],\n",
    "                subset[\"volume_ml\"],\n",
    "                label=dataset,\n",
    "                alpha=0.6,\n",
    "                s=100,\n",
    "                edgecolors=\"black\",\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "\n",
    "        axes[0].set_xlabel(\"Colon Length (cm)\", fontsize=12)\n",
    "        axes[0].set_ylabel(\"Colon Volume (ml)\", fontsize=12)\n",
    "        axes[0].set_title(\n",
    "            \"Length vs Volume Relationship\", fontsize=14, fontweight=\"bold\"\n",
    "        )\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(alpha=0.3)\n",
    "\n",
    "        # Average diameter distribution\n",
    "        for dataset in df[\"dataset_type\"].unique():\n",
    "            subset = df[df[\"dataset_type\"] == dataset][\"avg_diameter_mm\"].dropna()\n",
    "            axes[1].hist(\n",
    "                subset,\n",
    "                bins=20,\n",
    "                alpha=0.5,\n",
    "                label=dataset,\n",
    "                edgecolor=\"black\",\n",
    "                linewidth=0.5,\n",
    "            )\n",
    "\n",
    "        axes[1].set_xlabel(\"Average Colon Diameter (mm)\", fontsize=12)\n",
    "        axes[1].set_ylabel(\"Frequency\", fontsize=12)\n",
    "        axes[1].set_title(\n",
    "            \"Average Diameter Distribution\", fontsize=14, fontweight=\"bold\"\n",
    "        )\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(output_dir, \"05_volume_diameter_analysis.png\"),\n",
    "            dpi=300,\n",
    "            bbox_inches=\"tight\",\n",
    "        )\n",
    "        plt.close()\n",
    "\n",
    "    print(f\"\\nAll plots saved to '{output_dir}/' directory\")\n",
    "\n",
    "\n",
    "def generate_statistical_summary(df: pd.DataFrame, output_dir: str = \"plots\"):\n",
    "    \"\"\"\n",
    "    Generate comprehensive statistical summary and tests.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with measurements\n",
    "        output_dir: Output directory\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"COMPREHENSIVE STATISTICAL ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Overall summary statistics\n",
    "    summary = (\n",
    "        df.groupby(\"dataset_type\")\n",
    "        .agg(\n",
    "            {\n",
    "                \"length_cm\": [\"count\", \"mean\", \"std\", \"min\", \"median\", \"max\"],\n",
    "                \"volume_ml\": [\"mean\", \"std\", \"median\"],\n",
    "                \"avg_diameter_mm\": [\"mean\", \"std\", \"median\"],\n",
    "            }\n",
    "        )\n",
    "        .round(2)\n",
    "    )\n",
    "\n",
    "    print(\"\\n1. DESCRIPTIVE STATISTICS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(summary)\n",
    "\n",
    "    # Statistical tests for C-grade comparison\n",
    "    c_orig = df[df[\"dataset_type\"] == \"c_grade_original\"].set_index(\"patient_id\")\n",
    "    c_pred = df[df[\"dataset_type\"] == \"c_grade_predicted\"].set_index(\"patient_id\")\n",
    "\n",
    "    common_patients = c_orig.index.intersection(c_pred.index)\n",
    "\n",
    "    if len(common_patients) > 0:\n",
    "        orig_lengths = c_orig.loc[common_patients, \"length_cm\"].values\n",
    "        pred_lengths = c_pred.loc[common_patients, \"length_cm\"].values\n",
    "\n",
    "        # Paired t-test\n",
    "        t_stat, p_value = stats.ttest_rel(pred_lengths, orig_lengths)\n",
    "\n",
    "        # Wilcoxon signed-rank test (non-parametric alternative)\n",
    "        w_stat, w_p_value = stats.wilcoxon(pred_lengths, orig_lengths)\n",
    "\n",
    "        # Effect size (Cohen's d)\n",
    "        differences = pred_lengths - orig_lengths\n",
    "        cohens_d = np.mean(differences) / np.std(differences)\n",
    "\n",
    "        print(\"\\n2. PAIRED COMPARISON: C-GRADE ORIGINAL vs PREDICTED\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Number of paired samples: {len(common_patients)}\")\n",
    "        print(f\"Mean difference (Pred - Orig): {np.mean(differences):.2f} cm\")\n",
    "        print(f\"Std of differences: {np.std(differences):.2f} cm\")\n",
    "        print(f\"\\nPaired t-test:\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.4f}\")\n",
    "        print(f\"  Significant at =0.05: {'Yes' if p_value < 0.05 else 'No'}\")\n",
    "        print(f\"\\nWilcoxon signed-rank test:\")\n",
    "        print(f\"  W-statistic: {w_stat:.4f}\")\n",
    "        print(f\"  p-value: {w_p_value:.4f}\")\n",
    "        print(f\"\\nEffect size (Cohen's d): {cohens_d:.4f}\")\n",
    "\n",
    "        # Correlation\n",
    "        correlation = np.corrcoef(orig_lengths, pred_lengths)[0, 1]\n",
    "        print(f\"Pearson correlation: {correlation:.4f}\")\n",
    "\n",
    "        # Improvement metrics\n",
    "        improved = (differences > 0).sum()\n",
    "        worsened = (differences < 0).sum()\n",
    "        unchanged = (differences == 0).sum()\n",
    "\n",
    "        print(f\"\\n3. IMPROVEMENT ANALYSIS\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\n",
    "            f\"Cases with increased length: {improved} ({100*improved/len(differences):.1f}%)\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Cases with decreased length: {worsened} ({100*worsened/len(differences):.1f}%)\"\n",
    "        )\n",
    "        print(f\"Cases unchanged: {unchanged} ({100*unchanged/len(differences):.1f}%)\")\n",
    "\n",
    "        # Mean absolute error and relative error\n",
    "        mae = np.mean(np.abs(differences))\n",
    "        mape = np.mean(np.abs(differences) / orig_lengths) * 100\n",
    "\n",
    "        print(f\"\\nMean Absolute Error: {mae:.2f} cm\")\n",
    "        print(f\"Mean Absolute Percentage Error: {mape:.2f}%\")\n",
    "\n",
    "    # ANOVA across all groups\n",
    "    groups = [\n",
    "        df[df[\"dataset_type\"] == dt][\"length_cm\"].dropna().values\n",
    "        for dt in df[\"dataset_type\"].unique()\n",
    "    ]\n",
    "\n",
    "    if len(groups) > 2:\n",
    "        f_stat, anova_p = stats.f_oneway(*groups)\n",
    "\n",
    "        print(f\"\\n4. ONE-WAY ANOVA (ALL GROUPS)\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"F-statistic: {f_stat:.4f}\")\n",
    "        print(f\"p-value: {anova_p:.4f}\")\n",
    "        print(f\"Significant at =0.05: {'Yes' if anova_p < 0.05 else 'No'}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "\n",
    "    # Save summary to CSV\n",
    "    summary.to_csv(os.path.join(output_dir, \"statistical_summary.csv\"))\n",
    "\n",
    "    # Save detailed comparison if available\n",
    "    if len(common_patients) > 0:\n",
    "        comparison_df = pd.DataFrame(\n",
    "            {\n",
    "                \"patient_id\": common_patients,\n",
    "                \"original_length_cm\": orig_lengths,\n",
    "                \"predicted_length_cm\": pred_lengths,\n",
    "                \"difference_cm\": differences,\n",
    "                \"percent_change\": (differences / orig_lengths) * 100,\n",
    "            }\n",
    "        )\n",
    "        comparison_df.to_csv(\n",
    "            os.path.join(output_dir, \"c_grade_paired_comparison.csv\"), index=False\n",
    "        )\n",
    "        print(\n",
    "            f\"Detailed comparison saved to '{output_dir}/c_grade_paired_comparison.csv'\"\n",
    "        )\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution function integrating MONAI transforms.\n",
    "    \"\"\"\n",
    "    # ==================== CONFIGURATION ====================\n",
    "    # UPDATE THESE PATHS TO YOUR ACTUAL DATA\n",
    "    # TRAINING_JSONL = \"/path/to/training_data.jsonl\"\n",
    "    # C_GRADE_JSONL = \"/path/to/c_grade_data.jsonl\"\n",
    "    # PRED_DIR = \"/path/to/predictions\"\n",
    "    # OUTPUT_DIR = \"colon_analysis_results\"\n",
    "\n",
    "    TRAINING_JSONL = \"/home/yb107/cvpr2025/DukeDiffSeg/data/mobina_mixed_colon_dataset/mobina_mixed_colon_dataset_with_body_filled.jsonl\"\n",
    "    C_GRADE_JSONL = \"/home/yb107/cvpr2025/DukeDiffSeg/data/c_grade_colons/3d_vlsmv2_c_grade_colon_dataset_with_body_filled.jsonl\"\n",
    "    PRED_DIR = \"/home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-colon/5.1/inference_c_grade_550_gs_2.0_final_small_with_skeletonization\"\n",
    "    OUTPUT_DIR = \"/home/yb107/cvpr2025/DukeDiffSeg/notebook/length_analysis\"\n",
    "\n",
    "    BATCH_SIZE = 4\n",
    "    NUM_WORKERS = 4\n",
    "    # =======================================================\n",
    "\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COLON LENGTH ANALYSIS USING MONAI TRANSFORMS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Step 1: Load data dictionaries\n",
    "    print(\"\\n[1/5] Loading data dictionaries...\")\n",
    "\n",
    "    training_dicts = get_data_dicts_from_jsonl(TRAINING_JSONL, dataset_type=\"training\")\n",
    "    print(f\"  Loaded {len(training_dicts)} training cases\")\n",
    "\n",
    "    c_grade_dicts = get_data_dicts_from_jsonl(\n",
    "        C_GRADE_JSONL, dataset_type=\"c_grade_original\"\n",
    "    )\n",
    "    print(f\"  Loaded {len(c_grade_dicts)} C-grade original cases\")\n",
    "\n",
    "    c_pred_dicts = get_c_grade_prediction_dicts(C_GRADE_JSONL, PRED_DIR)\n",
    "    print(f\"  Loaded {len(c_pred_dicts)} C-grade prediction cases\")\n",
    "\n",
    "    # Step 2: Process datasets with MONAI\n",
    "    print(\"\\n[2/5] Processing datasets with MONAI transforms...\")\n",
    "\n",
    "    print(\"\\n  Processing training data...\")\n",
    "    df_training = process_dataset_with_monai(\n",
    "        training_dicts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        is_binary_pred=False,\n",
    "    )\n",
    "\n",
    "    print(\"\\n  Processing C-grade original data...\")\n",
    "    df_c_grade = process_dataset_with_monai(\n",
    "        c_grade_dicts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        is_binary_pred=False,\n",
    "    )\n",
    "\n",
    "    print(\"\\n  Processing C-grade predictions...\")\n",
    "    df_c_pred = process_dataset_with_monai(\n",
    "        c_pred_dicts,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        is_binary_pred=True,\n",
    "    )\n",
    "\n",
    "    # Step 3: Combine results\n",
    "    print(\"\\n[3/5] Combining results...\")\n",
    "    df_all = pd.concat([df_training, df_c_grade, df_c_pred], ignore_index=True)\n",
    "\n",
    "    # Save raw measurements\n",
    "    output_csv = os.path.join(OUTPUT_DIR, \"colon_measurements_all.csv\")\n",
    "    df_all.to_csv(output_csv, index=False)\n",
    "    print(f\"  Saved {len(df_all)} measurements to '{output_csv}'\")\n",
    "\n",
    "    # Step 4: Generate visualizations\n",
    "    print(\"\\n[4/5] Generating visualizations...\")\n",
    "    create_comprehensive_plots(df_all, output_dir=OUTPUT_DIR)\n",
    "\n",
    "    # Step 5: Statistical analysis\n",
    "    print(\"\\n[5/5] Performing statistical analysis...\")\n",
    "    generate_statistical_summary(df_all, output_dir=OUTPUT_DIR)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ANALYSIS COMPLETE!\")\n",
    "    print(f\"All results saved to '{OUTPUT_DIR}/' directory\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DukeDiffSeg-HooVw7aP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
