{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68ccc678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Sequence\n",
    "import math\n",
    "from monai.networks.nets import DiffusionModelUNet\n",
    "\n",
    "\n",
    "class FiLMLayer(nn.Module):\n",
    "    \"\"\"Applies Feature-wise Linear Modulation: out = gamma * x + beta\"\"\"\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, gamma: torch.Tensor, beta: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (B, C, D, H, W) feature maps\n",
    "            gamma: (B, C) scale parameters\n",
    "            beta: (B, C) shift parameters\n",
    "        Returns:\n",
    "            modulated features (B, C, D, H, W)\n",
    "        \"\"\"\n",
    "        # Reshape for broadcasting: (B, C) -> (B, C, 1, 1, 1)\n",
    "        gamma = gamma.view(gamma.shape[0], gamma.shape[1], 1, 1, 1)\n",
    "        beta = beta.view(beta.shape[0], beta.shape[1], 1, 1, 1)\n",
    "\n",
    "        return gamma * x + beta\n",
    "\n",
    "\n",
    "class FiLMAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Generates FiLM parameters (gamma, beta) from volume and spacing information.\n",
    "    Outputs modulation parameters for each U-Net resolution level.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet_channels: Sequence[int] = (32, 64, 64, 64),\n",
    "        embed_dim: int = 256,\n",
    "        volume_mean: float = 150.0,\n",
    "        volume_std: float = 100.0,\n",
    "        use_log_volume: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            unet_channels: Channel dimensions at each U-Net level (should match your U-Net)\n",
    "            embed_dim: Dimension of the intermediate embedding\n",
    "            volume_mean: Mean volume for normalization (compute from your dataset)\n",
    "            volume_std: Std volume for normalization (compute from your dataset)\n",
    "            use_log_volume: If True, use log normalization instead of standard normalization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.unet_channels = list(unet_channels)\n",
    "        self.volume_mean = volume_mean\n",
    "        self.volume_std = volume_std\n",
    "        self.use_log_volume = use_log_volume\n",
    "\n",
    "        # Base embedding network for volume + spacing\n",
    "        self.embedding_net = nn.Sequential(\n",
    "            nn.Linear(4, 64),  # 4 = volume (1) + spacing (3)\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, embed_dim),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # FiLM parameter generators for each U-Net level\n",
    "        # Each generates both gamma and beta (2 * channels)\n",
    "        self.film_generators = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(embed_dim, embed_dim),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(embed_dim, 2 * channels),\n",
    "                )\n",
    "                for channels in self.unet_channels\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Learnable scale factors for each level (helps with initialization)\n",
    "        self.gamma_scales = nn.ParameterList(\n",
    "            [\n",
    "                nn.Parameter(torch.ones(channels) * 0.1)\n",
    "                for channels in self.unet_channels\n",
    "            ]\n",
    "        )\n",
    "        self.beta_scales = nn.ParameterList(\n",
    "            [\n",
    "                nn.Parameter(torch.ones(channels) * 0.1)\n",
    "                for channels in self.unet_channels\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def normalize_volume(self, volume: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Normalize volume to a reasonable range\"\"\"\n",
    "        if self.use_log_volume:\n",
    "            # Log normalization (better for volumes with large variance)\n",
    "            volume = torch.log(volume + 1.0)\n",
    "            log_mean = torch.log(torch.tensor(self.volume_mean + 1.0))\n",
    "            log_std = torch.log(torch.tensor(self.volume_std + 1.0))\n",
    "            return (volume - log_mean) / log_std\n",
    "        else:\n",
    "            # Standard normalization\n",
    "            return (volume - self.volume_mean) / self.volume_std\n",
    "\n",
    "    def forward(\n",
    "        self, volume: torch.Tensor, spacing: torch.Tensor\n",
    "    ) -> list[tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            volume: (B,) or (B, 1) - organ volumes in ml\n",
    "            spacing: (B, 3) - [spacing_x, spacing_y, spacing_z] in mm\n",
    "\n",
    "        Returns:\n",
    "            List of (gamma, beta) tuples, one for each U-Net level\n",
    "            Each gamma and beta has shape (B, C) where C is channels at that level\n",
    "        \"\"\"\n",
    "        # Normalize volume\n",
    "        if volume.dim() == 1:\n",
    "            volume = volume.unsqueeze(-1)  # (B, 1)\n",
    "\n",
    "        volume_normalized = self.normalize_volume(volume)\n",
    "\n",
    "        # Concatenate volume and spacing\n",
    "        volume_spacing = torch.cat([volume_normalized, spacing], dim=-1)  # (B, 4)\n",
    "\n",
    "        # Generate base embedding\n",
    "        base_embed = self.embedding_net(volume_spacing)  # (B, embed_dim)\n",
    "\n",
    "        # Generate FiLM parameters for each level\n",
    "        film_params = []\n",
    "        for i, (film_gen, gamma_scale, beta_scale) in enumerate(\n",
    "            zip(self.film_generators, self.gamma_scales, self.beta_scales)\n",
    "        ):\n",
    "            params = film_gen(base_embed)  # (B, 2*C)\n",
    "            gamma_raw, beta_raw = params.chunk(2, dim=-1)  # Each (B, C)\n",
    "\n",
    "            # Scale and shift to initialize near identity transform\n",
    "            # gamma starts near 1.0, beta starts near 0.0\n",
    "            gamma = gamma_raw * gamma_scale + 1.0\n",
    "            beta = beta_raw * beta_scale\n",
    "\n",
    "            film_params.append((gamma, beta))\n",
    "\n",
    "        return film_params\n",
    "\n",
    "\n",
    "class DiffusionModelUNetFiLM(DiffusionModelUNet):\n",
    "    \"\"\"\n",
    "    DiffusionModelUNet with FiLM (Feature-wise Linear Modulation) conditioning.\n",
    "    Applies FiLM after each down block, middle block, and up block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        # FiLM layers for each resolution level\n",
    "        # down_blocks + middle + up_blocks\n",
    "        num_levels = len(self.block_out_channels)\n",
    "\n",
    "        # FiLM for down blocks\n",
    "        self.film_down = nn.ModuleList([FiLMLayer() for _ in range(num_levels)])\n",
    "\n",
    "        # FiLM for middle block\n",
    "        self.film_mid = FiLMLayer()\n",
    "\n",
    "        # FiLM for up blocks\n",
    "        self.film_up = nn.ModuleList([FiLMLayer() for _ in range(num_levels)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        timesteps: torch.Tensor,\n",
    "        film_params: list[tuple[torch.Tensor, torch.Tensor]] | None = None,\n",
    "        context: torch.Tensor | None = None,\n",
    "        class_labels: torch.Tensor | None = None,\n",
    "        down_block_additional_residuals: tuple[torch.Tensor] | None = None,\n",
    "        mid_block_additional_residual: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor (N, C, SpatialDims)\n",
    "            timesteps: timestep tensor (N,)\n",
    "            film_params: List of (gamma, beta) tuples for FiLM conditioning.\n",
    "                        Should have length = len(channels) for down blocks.\n",
    "                        If None, no FiLM conditioning is applied.\n",
    "            context: context tensor for cross-attention (N, 1, ContextDim)\n",
    "            class_labels: class labels (N,)\n",
    "            down_block_additional_residuals: additional residuals for controlnet\n",
    "            mid_block_additional_residual: additional residual for controlnet\n",
    "        \"\"\"\n",
    "        # 1. Time embedding\n",
    "        t_emb = get_timestep_embedding(timesteps, self.block_out_channels[0])\n",
    "        t_emb = t_emb.to(dtype=x.dtype)\n",
    "        emb = self.time_embed(t_emb)\n",
    "\n",
    "        # 2. Class embedding\n",
    "        if self.num_class_embeds is not None:\n",
    "            if class_labels is None:\n",
    "                raise ValueError(\n",
    "                    \"class_labels should be provided when num_class_embeds > 0\"\n",
    "                )\n",
    "            class_emb = self.class_embedding(class_labels)\n",
    "            class_emb = class_emb.to(dtype=x.dtype)\n",
    "            emb = emb + class_emb\n",
    "\n",
    "        # 3. Initial convolution\n",
    "        h = self.conv_in(x)\n",
    "\n",
    "        # 4. Down blocks with FiLM\n",
    "        if context is not None and self.with_conditioning is False:\n",
    "            raise ValueError(\n",
    "                \"model should have with_conditioning = True if context is provided\"\n",
    "            )\n",
    "\n",
    "        down_block_res_samples: list[torch.Tensor] = [h]\n",
    "        for i, downsample_block in enumerate(self.down_blocks):\n",
    "            h, res_samples = downsample_block(\n",
    "                hidden_states=h, temb=emb, context=context\n",
    "            )\n",
    "\n",
    "            # Apply FiLM modulation after the down block\n",
    "            if film_params is not None and i < len(film_params):\n",
    "                gamma, beta = film_params[i]\n",
    "                h = self.film_down[i](h, gamma, beta)\n",
    "\n",
    "                # Also apply FiLM to residual connections\n",
    "                res_samples = [\n",
    "                    self.film_down[i](res, gamma, beta) for res in res_samples\n",
    "                ]\n",
    "\n",
    "            for residual in res_samples:\n",
    "                down_block_res_samples.append(residual)\n",
    "\n",
    "        # Additional residuals for ControlNet\n",
    "        if down_block_additional_residuals is not None:\n",
    "            new_down_block_res_samples: list[torch.Tensor] = []\n",
    "            for down_block_res_sample, down_block_additional_residual in zip(\n",
    "                down_block_res_samples, down_block_additional_residuals\n",
    "            ):\n",
    "                down_block_res_sample = (\n",
    "                    down_block_res_sample + down_block_additional_residual\n",
    "                )\n",
    "                new_down_block_res_samples.append(down_block_res_sample)\n",
    "            down_block_res_samples = new_down_block_res_samples\n",
    "\n",
    "        # 5. Middle block with FiLM\n",
    "        h = self.middle_block(hidden_states=h, temb=emb, context=context)\n",
    "\n",
    "        if film_params is not None:\n",
    "            # Use the last down block's FiLM params for middle block\n",
    "            gamma, beta = film_params[-1]\n",
    "            h = self.film_mid(h, gamma, beta)\n",
    "\n",
    "        # Additional residual for ControlNet\n",
    "        if mid_block_additional_residual is not None:\n",
    "            h = h + mid_block_additional_residual\n",
    "\n",
    "        # 6. Up blocks with FiLM\n",
    "        for i, upsample_block in enumerate(self.up_blocks):\n",
    "            idx: int = -len(upsample_block.resnets)  # type: ignore\n",
    "            res_samples = down_block_res_samples[idx:]\n",
    "            down_block_res_samples = down_block_res_samples[:idx]\n",
    "\n",
    "            h = upsample_block(\n",
    "                hidden_states=h,\n",
    "                res_hidden_states_list=res_samples,\n",
    "                temb=emb,\n",
    "                context=context,\n",
    "            )\n",
    "\n",
    "            # Apply FiLM modulation after up block\n",
    "            # Use corresponding down block's parameters (mirrored)\n",
    "            if film_params is not None:\n",
    "                film_idx = len(self.up_blocks) - 1 - i  # Mirror the indices\n",
    "                if film_idx < len(film_params):\n",
    "                    gamma, beta = film_params[film_idx]\n",
    "                    h = self.film_up[i](h, gamma, beta)\n",
    "\n",
    "        # 7. Output block\n",
    "        output: torch.Tensor = self.out(h)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def get_timestep_embedding(\n",
    "    timesteps: torch.Tensor, embedding_dim: int, max_period: int = 10000\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings following the implementation in Ho et al. \"Denoising Diffusion Probabilistic\n",
    "    Models\" https://arxiv.org/abs/2006.11239.\n",
    "\n",
    "    Args:\n",
    "        timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "        embedding_dim: the dimension of the output.\n",
    "        max_period: controls the minimum frequency of the embeddings.\n",
    "    \"\"\"\n",
    "    if timesteps.ndim != 1:\n",
    "        raise ValueError(\"Timesteps should be a 1d-array\")\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    exponent = -math.log(max_period) * torch.arange(\n",
    "        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device\n",
    "    )\n",
    "    freqs = torch.exp(exponent / half_dim)\n",
    "\n",
    "    args = timesteps[:, None].float() * freqs[None, :]\n",
    "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "\n",
    "    # zero pad\n",
    "    if embedding_dim % 2 == 1:\n",
    "        embedding = torch.nn.functional.pad(embedding, (0, 1, 0, 0))\n",
    "\n",
    "    return embedding\n",
    "\n",
    "\n",
    "class FiLMAdapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Generates FiLM parameters (gamma, beta) from volume and spacing information.\n",
    "    Outputs modulation parameters for each U-Net resolution level.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        unet_channels: Sequence[int] = (32, 64, 64, 64),\n",
    "        embed_dim: int = 256,\n",
    "        volume_mean: float = 150.0,\n",
    "        volume_std: float = 100.0,\n",
    "        use_log_volume: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            unet_channels: Channel dimensions at each U-Net level (should match your U-Net)\n",
    "            embed_dim: Dimension of the intermediate embedding\n",
    "            volume_mean: Mean volume for normalization (compute from your dataset)\n",
    "            volume_std: Std volume for normalization (compute from your dataset)\n",
    "            use_log_volume: If True, use log normalization instead of standard normalization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.unet_channels = list(unet_channels)\n",
    "        self.volume_mean = volume_mean\n",
    "        self.volume_std = volume_std\n",
    "        self.use_log_volume = use_log_volume\n",
    "\n",
    "        # Base embedding network for volume + spacing\n",
    "        self.embedding_net = nn.Sequential(\n",
    "            nn.Linear(4, 64),  # 4 = volume (1) + spacing (3)\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, embed_dim),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # FiLM parameter generators for each U-Net level\n",
    "        # Each generates both gamma and beta (2 * channels)\n",
    "        self.film_generators = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(embed_dim, embed_dim),\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(embed_dim, 2 * channels),\n",
    "                )\n",
    "                for channels in self.unet_channels\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Learnable scale factors for each level (helps with initialization)\n",
    "        self.gamma_scales = nn.ParameterList(\n",
    "            [\n",
    "                nn.Parameter(torch.ones(channels) * 0.1)\n",
    "                for channels in self.unet_channels\n",
    "            ]\n",
    "        )\n",
    "        self.beta_scales = nn.ParameterList(\n",
    "            [\n",
    "                nn.Parameter(torch.ones(channels) * 0.1)\n",
    "                for channels in self.unet_channels\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def normalize_volume(self, volume: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Normalize volume to a reasonable range\"\"\"\n",
    "        if self.use_log_volume:\n",
    "            # Log normalization (better for volumes with large variance)\n",
    "            volume = torch.log(volume + 1.0)\n",
    "            log_mean = torch.log(torch.tensor(self.volume_mean + 1.0))\n",
    "            log_std = torch.log(torch.tensor(self.volume_std + 1.0))\n",
    "            return (volume - log_mean) / log_std\n",
    "        else:\n",
    "            # Standard normalization\n",
    "            return (volume - self.volume_mean) / self.volume_std\n",
    "\n",
    "    def forward(\n",
    "        self, volume: torch.Tensor, spacing: torch.Tensor\n",
    "    ) -> list[tuple[torch.Tensor, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            volume: (B,) or (B, 1) - organ volumes in ml\n",
    "            spacing: (B, 3) - [spacing_x, spacing_y, spacing_z] in mm\n",
    "\n",
    "        Returns:\n",
    "            List of (gamma, beta) tuples, one for each U-Net level\n",
    "            Each gamma and beta has shape (B, C) where C is channels at that level\n",
    "        \"\"\"\n",
    "        # Normalize volume\n",
    "        if volume.dim() == 1:\n",
    "            volume = volume.unsqueeze(-1)  # (B, 1)\n",
    "\n",
    "        volume_normalized = self.normalize_volume(volume)\n",
    "\n",
    "        # Concatenate volume and spacing\n",
    "        volume_spacing = torch.cat([volume_normalized, spacing], dim=-1)  # (B, 4)\n",
    "\n",
    "        # Generate base embedding\n",
    "        base_embed = self.embedding_net(volume_spacing)  # (B, embed_dim)\n",
    "\n",
    "        # Generate FiLM parameters for each level\n",
    "        film_params = []\n",
    "        for i, (film_gen, gamma_scale, beta_scale) in enumerate(\n",
    "            zip(self.film_generators, self.gamma_scales, self.beta_scales)\n",
    "        ):\n",
    "            params = film_gen(base_embed)  # (B, 2*C)\n",
    "            gamma_raw, beta_raw = params.chunk(2, dim=-1)  # Each (B, C)\n",
    "\n",
    "            # Scale and shift to initialize near identity transform\n",
    "            # gamma starts near 1.0, beta starts near 0.0\n",
    "            gamma = gamma_raw * gamma_scale + 1.0\n",
    "            beta = beta_raw * beta_scale\n",
    "\n",
    "            film_params.append((gamma, beta))\n",
    "\n",
    "        return film_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1255a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MINIMAL INFERENCE SETUP FOR JUPYTER NOTEBOOK\n",
    "# ============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import monai\n",
    "from monai import transforms\n",
    "from monai.networks.nets import DiffusionModelUNet\n",
    "from monai.networks.schedulers import DDIMScheduler, DDPMScheduler\n",
    "\n",
    "# Import your custom utilities (adjust paths as needed)\n",
    "from utils.data import MaskToSDFd, sdf_to_mask\n",
    "from utils.monai_transforms import (\n",
    "    HarmonizeLabelsd,\n",
    "    AddSpacingTensord,\n",
    "    FilterAndRelabeld,\n",
    "    EnsureAllTorchd,\n",
    "    CropForegroundAxisd,\n",
    ")\n",
    "\n",
    "from monai.transforms import Transform\n",
    "\n",
    "\n",
    "class ProbeTransform(Transform):\n",
    "    def __init__(self, message=\"ProbeTransform called\"):\n",
    "        super().__init__()\n",
    "        self.message = message\n",
    "\n",
    "    def __call__(self, data):\n",
    "        print(self.message)\n",
    "        return data\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 1. CONFIGURATION\n",
    "# ============================================================================\n",
    "class VolumeSpacingEmbedding(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(4, 64),  # 4 = volume (1) + spacing (3)\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(embed_dim, embed_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, volume, spacing):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            volume: (B,) or (B, 1) - organ volume in ml\n",
    "            spacing: (B, 3) - [spacing_x, spacing_y, spacing_z] in mm\n",
    "        Returns:\n",
    "            embedding: (B, embed_dim)\n",
    "        \"\"\"\n",
    "        if volume.dim() == 1:\n",
    "            volume = volume.unsqueeze(-1)  # (B, 1)\n",
    "        # Concatenate volume and spacing\n",
    "        volume_spacing = torch.cat([volume, spacing], dim=-1)  # (B, 4)\n",
    "        return self.mlp(volume_spacing)\n",
    "\n",
    "\n",
    "class InferenceConfig:\n",
    "    # Model params\n",
    "    spatial_dims = 3\n",
    "    in_channels = 1  # image SDF + conditioning\n",
    "    out_channels = 1  # target organ SDF\n",
    "    features = [32, 64, 64, 128, 256]  # adjust based on your trained model\n",
    "    attention_levels = [False, False, False, False, False]\n",
    "    num_head_channels = [0, 0, 0, 64, 64]\n",
    "    with_conditioning = True\n",
    "    cross_attention_dim = 256  # adjust based on your trained model\n",
    "    volume_embedding_dim = 128\n",
    "\n",
    "    # Diffusion params\n",
    "    diffusion_steps = 1000\n",
    "    ddim_steps = 20\n",
    "    beta_schedule = \"scaled_linear_beta\"\n",
    "    model_mean_type = \"sample\"  # or \"sample\"\n",
    "    guidance_scale = 1.0  # CFG scale\n",
    "    condition_drop_prob = 0.1\n",
    "\n",
    "    # Data params\n",
    "    pixdim = (1.5, 1.5, 2.0)\n",
    "    orientation = \"RAS\"\n",
    "    roi_size = (128, 128, 128)\n",
    "\n",
    "    # Paths\n",
    "    checkpoint_path = None\n",
    "    # checkpoint_path = \"/home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-iterative/7.2/DiffUnet-binary-iterative_liver_latest_checkpoint_97.pt\"\n",
    "    device = \"cuda:1\"\n",
    "\n",
    "\n",
    "config = InferenceConfig()\n",
    "\n",
    "# ============================================================================\n",
    "# 2. ORGAN MAPPING (from your script)\n",
    "# ============================================================================\n",
    "\n",
    "ORGAN_NAMES = {\n",
    "    1: \"colon\",\n",
    "    2: \"rectum\",\n",
    "    3: \"small_bowel\",\n",
    "    4: \"stomach\",\n",
    "    5: \"liver\",\n",
    "    6: \"spleen\",\n",
    "    7: \"kidneys\",\n",
    "    9: \"pancreas\",\n",
    "    10: \"urinary_bladder\",\n",
    "    11: \"duodenum\",\n",
    "    12: \"gallbladder\",\n",
    "}\n",
    "NAME_TO_INDEX = {v: k for k, v in ORGAN_NAMES.items()}\n",
    "\n",
    "\n",
    "def get_conditioning_organs(generation_order, target_organ_index):\n",
    "    \"\"\"Get list of organs to condition on\"\"\"\n",
    "    if target_organ_index not in generation_order:\n",
    "        raise ValueError(f\"Target organ {target_organ_index} not in order\")\n",
    "    pos = generation_order.index(target_organ_index)\n",
    "    return generation_order[:pos]\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 3. BUILD PREPROCESSING TRANSFORM\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def build_inference_transform(config, target_organ=\"liver\", generation_order=None):\n",
    "    \"\"\"Simplified transform for single-sample inference\"\"\"\n",
    "\n",
    "    target_organ_index = NAME_TO_INDEX.get(target_organ)\n",
    "    if generation_order is None:\n",
    "        generation_order = [5, 6, 7, 9, 3, 1, 2, 4, 10, 11, 12]  # default order\n",
    "\n",
    "    conditioning_organs = get_conditioning_organs(generation_order, target_organ_index)\n",
    "\n",
    "    data_keys = [\"image\", \"label\", \"body_filled_channel\"]\n",
    "\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=data_keys),\n",
    "            transforms.EnsureChannelFirstd(keys=data_keys),\n",
    "            transforms.Spacingd(keys=data_keys, pixdim=config.pixdim, mode=\"nearest\"),\n",
    "            transforms.Orientationd(keys=data_keys, axcodes=config.orientation),\n",
    "            ProbeTransform(message=\"üêî After Orientationd\"),\n",
    "            # transforms.KeepLargestConnectedComponentd(keys=data_keys),\n",
    "            # ProbeTransform(message=\"üê∏ After KeepLargestConnectedComponentd\"),\n",
    "            HarmonizeLabelsd(keys=[\"image\", \"label\"], kidneys_same_index=True),\n",
    "            CropForegroundAxisd(\n",
    "                keys=data_keys,\n",
    "                source_key=\"image\",\n",
    "                axis=2,\n",
    "                margin=5,\n",
    "            ),\n",
    "            transforms.CropForegroundd(\n",
    "                keys=data_keys, source_key=\"body_filled_channel\", margin=5\n",
    "            ),\n",
    "            ProbeTransform(message=\"üê¢ After CropForegroundd\"),\n",
    "            transforms.Resized(\n",
    "                keys=data_keys, spatial_size=config.roi_size, mode=\"nearest\"\n",
    "            ),\n",
    "            AddSpacingTensord(ref_key=\"image\"),\n",
    "            FilterAndRelabeld(\n",
    "                image_key=\"image\",\n",
    "                label_key=\"label\",\n",
    "                conditioning_organs=conditioning_organs,\n",
    "                target_organ=target_organ_index,\n",
    "            ),\n",
    "            ProbeTransform(message=\"üêç After FilterAndRelabeld\"),\n",
    "            MaskToSDFd(\n",
    "                keys=data_keys,\n",
    "                spacing_key=\"spacing_tensor\",\n",
    "                device=torch.device(\"cpu\"),\n",
    "            ),\n",
    "            ProbeTransform(message=\"üêô After MaskToSDFd\"),\n",
    "            EnsureAllTorchd(print_changes=False),\n",
    "            transforms.EnsureTyped(\n",
    "                keys=data_keys + [\"spacing_tensor\"],\n",
    "                track_meta=True,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return transform\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# 4. BUILD MODEL\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "def build_model(config):\n",
    "    \"\"\"Create and load model\"\"\"\n",
    "\n",
    "    model = DiffusionModelUNetFiLM(\n",
    "        spatial_dims=config.spatial_dims,\n",
    "        in_channels=config.in_channels\n",
    "        + config.out_channels,  # concat image during inference\n",
    "        out_channels=config.out_channels,\n",
    "        channels=config.features,\n",
    "        attention_levels=config.attention_levels,\n",
    "        num_res_blocks=1,\n",
    "        transformer_num_layers=0,\n",
    "        num_head_channels=config.num_head_channels,\n",
    "        with_conditioning=False,\n",
    "        cross_attention_dim=None,\n",
    "    )\n",
    "    vol_embed = FiLMAdapter(\n",
    "        unet_channels=config.features,\n",
    "        embed_dim=256,\n",
    "        volume_mean=1625.45,\n",
    "        volume_std=535.51,\n",
    "        use_log_volume=True,\n",
    "    )\n",
    "\n",
    "    # vol_embed = VolumeSpacingEmbedding(embed_dim=config.volume_embedding_dim)\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(config.checkpoint_path, map_location=\"cpu\")\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    vol_embed.load_state_dict(checkpoint[\"vol_embed_net\"])\n",
    "\n",
    "    model = model.to(config.device)\n",
    "    vol_embed = vol_embed.to(config.device)\n",
    "    model.eval()\n",
    "    vol_embed.eval()\n",
    "\n",
    "    print(f\"‚úì Model loaded from {config.checkpoint_path}\")\n",
    "    print(f\"‚úì Parameters: {sum(p.numel() for p in model.parameters())/1e6:.2f}M\")\n",
    "\n",
    "    return model, vol_embed\n",
    "\n",
    "\n",
    "def build_scheduler(config):\n",
    "    \"\"\"Create DDIM scheduler for inference\"\"\"\n",
    "\n",
    "    scheduler = DDIMScheduler(\n",
    "        num_train_timesteps=config.diffusion_steps,\n",
    "        beta_start=0.0001,\n",
    "        beta_end=0.02,\n",
    "        schedule=config.beta_schedule,\n",
    "        clip_sample=False,\n",
    "        prediction_type=config.model_mean_type,\n",
    "    )\n",
    "    scheduler.set_timesteps(num_inference_steps=config.ddim_steps)\n",
    "\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_inference(\n",
    "    model, vol_embed, scheduler, image_sdf, body_filled_sdf, volume, spacing, config\n",
    "):\n",
    "    \"\"\"\n",
    "    Run DDIM sampling to generate organ mask\n",
    "\n",
    "    Args:\n",
    "        model: DiffusionModelUNet\n",
    "        scheduler: DDIMScheduler\n",
    "        image_sdf: conditioning image SDF [B, 1, H, W, D]\n",
    "        config: InferenceConfig\n",
    "\n",
    "    Returns:\n",
    "        pred_mask: binary mask [B, 1, H, W, D]\n",
    "        pred_sdf: signed distance field [B, 1, H, W, D]\n",
    "    \"\"\"\n",
    "\n",
    "    device = config.device\n",
    "    image_sdf = image_sdf.to(device).float()\n",
    "    pred = torch.randn_like(image_sdf)\n",
    "\n",
    "    # Initialize with random noise\n",
    "    if body_filled_sdf is not None:\n",
    "        body_filled_sdf = body_filled_sdf.to(device).float()\n",
    "        image = torch.cat([image_sdf, body_filled_sdf], dim=1)\n",
    "    else:\n",
    "        image = image_sdf\n",
    "\n",
    "    # vol_context = vol_embed(volume.to(device), spacing.to(device))\n",
    "    flim_params = vol_embed(volume.to(device), spacing.to(device))\n",
    "\n",
    "    # Get all timesteps\n",
    "    all_next_timesteps = torch.cat(\n",
    "        [scheduler.timesteps[1:], torch.tensor([0], dtype=scheduler.timesteps.dtype)]\n",
    "    )\n",
    "\n",
    "    # DDIM sampling loop\n",
    "    for i, (t, next_t) in enumerate(zip(scheduler.timesteps, all_next_timesteps)):\n",
    "        pred_before = pred.clone()\n",
    "        # Concatenate conditioning\n",
    "        model_input = torch.cat([pred, image], dim=1)\n",
    "\n",
    "        # Predict\n",
    "        # t_tensor = torch.full((image.shape[0],), t, device=device).long()\n",
    "        # model_output = model(x=model_input, timesteps=t_tensor, context=vol_context)\n",
    "        model_output = model(\n",
    "            x=model_input,\n",
    "            timesteps=torch.Tensor((t,)).to(image.device),\n",
    "            context=None,\n",
    "            film_params=flim_params,\n",
    "        )\n",
    "\n",
    "        print(f\"Model output mean at step {i}: {model_output.mean().item():.4f}\")\n",
    "\n",
    "        # Classifier-free guidance (if scale != 1.0)\n",
    "        if config.guidance_scale != 1.0:\n",
    "            image_uncond = torch.zeros_like(image)\n",
    "            model_input_uncond = torch.cat([pred, image_uncond], dim=1)\n",
    "            uncond_output = model(\n",
    "                x=model_input_uncond,\n",
    "                timesteps=torch.Tensor((t,)).to(image.device),\n",
    "                context=vol_embed,\n",
    "            )\n",
    "            model_output = uncond_output + config.guidance_scale * (\n",
    "                model_output - uncond_output\n",
    "            )\n",
    "\n",
    "        # DDIM step\n",
    "        pred, _ = scheduler.step(model_output, t, pred)\n",
    "\n",
    "        change = (pred - pred_before).abs().mean().item()\n",
    "        print(f\"Step {i}: t={t}, change={change:.6f}, pred_mean={pred.mean():.4f}\")\n",
    "\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"  Step {i+1}/{len(scheduler.timesteps)}\")\n",
    "\n",
    "    # Convert SDF to mask\n",
    "    pred_sdf = pred.clone()\n",
    "    pred_mask = sdf_to_mask(pred * 10.0)  # scale factor from your training\n",
    "\n",
    "    return pred_mask, pred_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba50b0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jsonl_path = \"/home/yb107/cvpr2025/DukeDiffSeg/data/mobina_mixed_colon_dataset/mobina_mixed_colon_dataset_with_body_filled_test.jsonl\"\n",
    "import json\n",
    "\n",
    "\n",
    "def load_jsonl_inference(jsonl_path):\n",
    "    data = []\n",
    "    with open(jsonl_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n",
    "test_data = load_jsonl_inference(test_jsonl_path)\n",
    "test_data = test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d3260aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Preprocessing data...\n",
      "üèãÔ∏è‚Äç‚ôÄÔ∏è Applying transforms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yb107/.local/share/virtualenvs/DukeDiffSeg-HooVw7aP/lib/python3.10/site-packages/monai/utils/deprecate_utils.py:321: FutureWarning: monai.transforms.spatial.dictionary Orientationd.__init__:labels: Current default value of argument `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` was changed in version None from `labels=(('L', 'R'), ('P', 'A'), ('I', 'S'))` to `labels=None`. Default value changed to None meaning that the transform now uses the 'space' of a meta-tensor, if applicable, to determine appropriate axis labels.\n",
      "  warn_deprecated(argname, msg, warning_category)\n"
     ]
    }
   ],
   "source": [
    "# 1. Preprocess\n",
    "config = InferenceConfig()\n",
    "# config.checkpoint_path = \"/home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-iterative/7.1/checkpoints/final_unet.pth\"\n",
    "config.checkpoint_path = \"/home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-iterative/7.5/checkpoints/liver/DiffUnet-binary-iterative_liver_latest_checkpoint_65.pt\"\n",
    "config.in_channels = 1\n",
    "\n",
    "print(\"üì¶ Preprocessing data...\")\n",
    "transform = build_inference_transform(\n",
    "    config, \"liver\", [5, 12, 6, 7, 4, 9, 11, 10, 2, 1, 3]\n",
    ")\n",
    "\n",
    "data_dict = {\n",
    "    \"image\": test_data[\"mask\"],\n",
    "    \"label\": test_data[\"mask\"],\n",
    "    \"body_filled_channel\": test_data[\"body_filled_mask\"],\n",
    "}\n",
    "print(\"üèãÔ∏è‚Äç‚ôÄÔ∏è Applying transforms...\")\n",
    "# data_dict = transform(data_dict)\n",
    "\n",
    "# SAVE data_dict as .pt for caching\n",
    "# torch.save(data_dict, \"tmp/data_dict.pt\")\n",
    "\n",
    "# load data_dict from .pt\n",
    "data_dict = torch.load(\"tmp/data_dict.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e7f59ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  Building model...\n",
      "‚úì Model loaded from /home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-iterative/7.5/checkpoints/liver/DiffUnet-binary-iterative_liver_latest_checkpoint_65.pt\n",
      "‚úì Parameters: 29.11M\n"
     ]
    }
   ],
   "source": [
    "# 2. Build model & scheduler\n",
    "print(\"üèóÔ∏è  Building model...\")\n",
    "model, vol_net = build_model(config)\n",
    "scheduler = build_scheduler(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c5feb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metatensor([1685.4402])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_sdf = data_dict[\"image\"].unsqueeze(0)  # [1, 1, H, W, D]\n",
    "label_sdf = data_dict[\"label\"].unsqueeze(0)\n",
    "label_mask = sdf_to_mask(label_sdf)\n",
    "body_filled_sdf = data_dict[\"body_filled_channel\"].unsqueeze(0)\n",
    "zero_filled_sdf = torch.zeros_like(body_filled_sdf)\n",
    "spacing = data_dict[\"spacing_tensor\"].unsqueeze(0)\n",
    "\n",
    "voxel_volume = spacing.prod(dim=1)\n",
    "label_volume = label_mask.sum(dim=(1, 2, 3, 4)) * voxel_volume\n",
    "label_volume = label_volume / 1000.0  # convert to ml\n",
    "label_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7890adc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Running DDIM sampling...\n",
      "Model output mean at step 0: -0.5166\n",
      "Step 0: t=950, change=0.012404, pred_mean=-0.0127\n",
      "Model output mean at step 1: -0.5168\n",
      "Step 1: t=900, change=0.016991, pred_mean=-0.0296\n",
      "Model output mean at step 2: -0.5169\n",
      "Step 2: t=850, change=0.022214, pred_mean=-0.0517\n",
      "Model output mean at step 3: -0.5166\n",
      "Step 3: t=800, change=0.027751, pred_mean=-0.0792\n",
      "Model output mean at step 4: -0.5162\n",
      "Step 4: t=750, change=0.033291, pred_mean=-0.1121\n",
      "  Step 5/20\n",
      "Model output mean at step 5: -0.5159\n",
      "Step 5: t=700, change=0.038782, pred_mean=-0.1497\n",
      "Model output mean at step 6: -0.5156\n",
      "Step 6: t=650, change=0.044451, pred_mean=-0.1910\n",
      "Model output mean at step 7: -0.5156\n",
      "Step 7: t=600, change=0.050418, pred_mean=-0.2346\n",
      "Model output mean at step 8: -0.5155\n",
      "Step 8: t=550, change=0.056538, pred_mean=-0.2789\n",
      "Model output mean at step 9: -0.5155\n",
      "Step 9: t=500, change=0.062423, pred_mean=-0.3222\n",
      "  Step 10/20\n",
      "Model output mean at step 10: -0.5156\n",
      "Step 10: t=450, change=0.067571, pred_mean=-0.3628\n",
      "Model output mean at step 11: -0.5157\n",
      "Step 11: t=400, change=0.071488, pred_mean=-0.3995\n",
      "Model output mean at step 12: -0.5158\n",
      "Step 12: t=350, change=0.073800, pred_mean=-0.4313\n",
      "Model output mean at step 13: -0.5159\n",
      "Step 13: t=300, change=0.074273, pred_mean=-0.4577\n",
      "Model output mean at step 14: -0.5160\n",
      "Step 14: t=250, change=0.072842, pred_mean=-0.4785\n",
      "  Step 15/20\n",
      "Model output mean at step 15: -0.5161\n",
      "Step 15: t=200, change=0.069600, pred_mean=-0.4940\n",
      "Model output mean at step 16: -0.5161\n",
      "Step 16: t=150, change=0.064852, pred_mean=-0.5047\n",
      "Model output mean at step 17: -0.5162\n",
      "Step 17: t=100, change=0.059626, pred_mean=-0.5116\n",
      "Model output mean at step 18: -0.5161\n",
      "Step 18: t=50, change=0.069159, pred_mean=-0.5158\n",
      "Model output mean at step 19: -0.5160\n",
      "Step 19: t=0, change=0.011537, pred_mean=-0.5160\n",
      "  Step 20/20\n"
     ]
    }
   ],
   "source": [
    "print(\"üé® Running DDIM sampling...\")\n",
    "pred_mask, pred_sdf = run_inference(\n",
    "    model,\n",
    "    vol_net,\n",
    "    scheduler,\n",
    "    image_sdf,\n",
    "    None,\n",
    "    label_volume,\n",
    "    spacing,\n",
    "    config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "64d64f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "metatensor([861.6476])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate pred_mask original volume\n",
    "pred_mask_volume = pred_mask.sum(dim=(1, 2, 3, 4)).cpu() * voxel_volume\n",
    "pred_mask_volume / 1000.0  # convert to ml\n",
    "\n",
    "# pred_sdf_logits = torch.sigmoid(pred_sdf * 10)  # scale factor from your training\n",
    "# pred_sdf_logits_volume = (pred_sdf_logits).sum(dim=(1, 2, 3, 4)).cpu() * voxel_volume\n",
    "# pred_sdf_logits_volume / 1000.0  # convert to ml\n",
    "\n",
    "# pred_sdf_logits.mean(), pred_sdf_logits.std(), pred_sdf_logits.min(), pred_sdf_logits.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b51858ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-08 21:24:47,350 INFO image_writer.py:197 - writing: tmp/Patient_00074_Study_78614_Series_03__pred_75.nii.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "metatensor([[[[0.1673, 0.0775, 0.0510,  ..., 0.0462, 0.0639, 0.1503],\n",
       "          [0.0795, 0.0242, 0.0133,  ..., 0.0121, 0.0191, 0.0709],\n",
       "          [0.0564, 0.0151, 0.0079,  ..., 0.0074, 0.0122, 0.0528],\n",
       "          ...,\n",
       "          [0.0508, 0.0127, 0.0065,  ..., 0.0067, 0.0113, 0.0496],\n",
       "          [0.0716, 0.0211, 0.0120,  ..., 0.0126, 0.0197, 0.0710],\n",
       "          [0.1565, 0.0726, 0.0508,  ..., 0.0529, 0.0719, 0.1590]],\n",
       "\n",
       "         [[0.0782, 0.0226, 0.0120,  ..., 0.0097, 0.0156, 0.0627],\n",
       "          [0.0243, 0.0041, 0.0018,  ..., 0.0014, 0.0026, 0.0186],\n",
       "          [0.0150, 0.0023, 0.0010,  ..., 0.0008, 0.0014, 0.0120],\n",
       "          ...,\n",
       "          [0.0123, 0.0017, 0.0007,  ..., 0.0006, 0.0012, 0.0105],\n",
       "          [0.0202, 0.0034, 0.0016,  ..., 0.0015, 0.0026, 0.0181],\n",
       "          [0.0701, 0.0209, 0.0123,  ..., 0.0123, 0.0189, 0.0685]],\n",
       "\n",
       "         [[0.0551, 0.0134, 0.0066,  ..., 0.0051, 0.0085, 0.0420],\n",
       "          [0.0149, 0.0022, 0.0009,  ..., 0.0007, 0.0012, 0.0106],\n",
       "          [0.0091, 0.0013, 0.0005,  ..., 0.0004, 0.0007, 0.0065],\n",
       "          ...,\n",
       "          [0.0072, 0.0009, 0.0004,  ..., 0.0003, 0.0005, 0.0054],\n",
       "          [0.0124, 0.0018, 0.0008,  ..., 0.0007, 0.0012, 0.0100],\n",
       "          [0.0503, 0.0131, 0.0073,  ..., 0.0069, 0.0108, 0.0464]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[0.0544, 0.0130, 0.0063,  ..., 0.0057, 0.0092, 0.0427],\n",
       "          [0.0143, 0.0020, 0.0008,  ..., 0.0006, 0.0010, 0.0092],\n",
       "          [0.0085, 0.0011, 0.0005,  ..., 0.0003, 0.0004, 0.0047],\n",
       "          ...,\n",
       "          [0.0077, 0.0009, 0.0004,  ..., 0.0003, 0.0005, 0.0048],\n",
       "          [0.0137, 0.0019, 0.0009,  ..., 0.0006, 0.0010, 0.0089],\n",
       "          [0.0532, 0.0133, 0.0075,  ..., 0.0061, 0.0099, 0.0437]],\n",
       "\n",
       "         [[0.0707, 0.0188, 0.0099,  ..., 0.0080, 0.0131, 0.0556],\n",
       "          [0.0205, 0.0031, 0.0014,  ..., 0.0008, 0.0016, 0.0133],\n",
       "          [0.0127, 0.0017, 0.0008,  ..., 0.0004, 0.0007, 0.0071],\n",
       "          ...,\n",
       "          [0.0118, 0.0014, 0.0006,  ..., 0.0004, 0.0008, 0.0077],\n",
       "          [0.0198, 0.0029, 0.0014,  ..., 0.0008, 0.0017, 0.0138],\n",
       "          [0.0688, 0.0187, 0.0108,  ..., 0.0087, 0.0145, 0.0591]],\n",
       "\n",
       "         [[0.1486, 0.0621, 0.0404,  ..., 0.0346, 0.0509, 0.1318],\n",
       "          [0.0654, 0.0171, 0.0094,  ..., 0.0066, 0.0114, 0.0520],\n",
       "          [0.0470, 0.0109, 0.0059,  ..., 0.0033, 0.0060, 0.0338],\n",
       "          ...,\n",
       "          [0.0444, 0.0093, 0.0048,  ..., 0.0040, 0.0072, 0.0372],\n",
       "          [0.0642, 0.0162, 0.0089,  ..., 0.0073, 0.0128, 0.0555],\n",
       "          [0.1464, 0.0613, 0.0414,  ..., 0.0377, 0.0557, 0.1393]]]],\n",
       "       device='cuda:1')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monai.transforms.SaveImage(\n",
    "    output_dir=\"tmp/\",\n",
    "    output_postfix=\"_pred_75\",\n",
    "    separate_folder=False,\n",
    "    # )(pred_mask.squeeze(0))\n",
    ")(pred_sdf_logits.squeeze(0))\n",
    "# monai.transforms.SaveImage(\n",
    "#     output_dir=\"tmp/\",\n",
    "#     output_postfix=\"_pred_sdf\",\n",
    "#     separate_folder=False,\n",
    "# )(pred_sdf.squeeze(0))\n",
    "# monai.transforms.SaveImage(\n",
    "#     output_dir=\"tmp/\",\n",
    "#     output_postfix=\"_img_sdf\",\n",
    "#     separate_folder=False,\n",
    "# )(label_sdf.squeeze(0))\n",
    "# monai.transforms.SaveImage(\n",
    "#     output_dir=\"tmp/\",\n",
    "#     output_postfix=\"_body_filled_sdf\",\n",
    "#     separate_folder=False,\n",
    "# )(body_filled_sdf.squeeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DukeDiffSeg-HooVw7aP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
