{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4192585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in JSONL: 356\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mask': '/data/usr/yb107/colon_data/refined_by_mobina/male_cases_refined_by_md/masks/Patient_01799_Study_07874_Series_03.nii.gz',\n",
       " 'body_filled_mask': '/data/usr/yb107/colon_data/refined_by_mobina/Body_filled_all/Patient_01799_Study_07874_Series_03_Body_filled.nii.gz'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import monai\n",
    "import json, os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from monai import transforms\n",
    "\n",
    "jsonl_path = \"/home/yb107/cvpr2025/DukeDiffSeg/data/mobina_mixed_colon_dataset/mobina_mixed_colon_dataset_with_body_filled.jsonl_train.jsonl\"\n",
    "# Load the JSONL file and create a list of dictionaries\n",
    "with open(jsonl_path, \"r\") as f:\n",
    "    files = [json.loads(line) for line in f]\n",
    "print(f\"Number of entries in JSONL: {len(files)}\")\n",
    "file = files[0]\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95c43c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from monai.transforms import MapTransform\n",
    "\n",
    "\n",
    "class CropForegroundAxisd(MapTransform):\n",
    "    \"\"\"\n",
    "    Crop the tensors in `keys` along a single spatial axis based on the foreground\n",
    "    of `source_key`. Other axes are left untouched.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, keys, source_key, axis=0, select_fn=lambda x: x > 0, margin=5):\n",
    "        if not isinstance(keys, (list, tuple)):\n",
    "            keys = [keys]\n",
    "        super().__init__(keys)\n",
    "        if axis not in (0, 1, 2):\n",
    "            raise ValueError(f\"`axis` must be 0, 1, or 2; got {axis}\")\n",
    "        if margin < 0:\n",
    "            raise ValueError(\"`margin` must be >= 0\")\n",
    "        self.keys = list(keys)\n",
    "        self.source_key = source_key\n",
    "        self.axis = axis\n",
    "        self.select_fn = select_fn\n",
    "        self.margin = margin\n",
    "\n",
    "    def _to_tensor(self, x):\n",
    "        return x if isinstance(x, torch.Tensor) else torch.as_tensor(x)\n",
    "\n",
    "    def _get_spatial_axis_index(self, arr_ndim: int) -> int:\n",
    "        if arr_ndim < 3:\n",
    "            raise ValueError(\n",
    "                f\"Input must have at least 3 dims (D,H,W). Got ndim={arr_ndim}\"\n",
    "            )\n",
    "        # spatial dims are the last 3 dims\n",
    "        return arr_ndim - 3 + self.axis\n",
    "\n",
    "    def _compute_crop_indices(self, src):\n",
    "        t = self._to_tensor(src)\n",
    "\n",
    "        # Reduce all non-spatial dims to a 3D spatial volume (D,H,W)\n",
    "        if t.ndim == 3:\n",
    "            spatial = t\n",
    "        else:\n",
    "            n_spatial = 3\n",
    "            reduce_dims = tuple(range(t.ndim - n_spatial))  # e.g., (0,) for C,D,H,W\n",
    "            spatial = t.any(dim=reduce_dims).to(t.dtype)\n",
    "\n",
    "        mask = self.select_fn(spatial)\n",
    "        mask = mask if isinstance(mask, torch.Tensor) else torch.as_tensor(mask)\n",
    "        mask = mask.bool()\n",
    "\n",
    "        if mask.ndim != 3:\n",
    "            raise ValueError(f\"Foreground mask must be 3D; got {tuple(mask.shape)}\")\n",
    "\n",
    "        axis = self.axis\n",
    "        other = tuple(d for d in (0, 1, 2) if d != axis)\n",
    "        # ↓↓↓ fix: reduce both non-axis dims at once to get a 1D presence vector\n",
    "        presence_1d = mask.any(dim=other)\n",
    "\n",
    "        if not presence_1d.any():\n",
    "            return None\n",
    "\n",
    "        idxs = presence_1d.nonzero(as_tuple=False).squeeze(-1)\n",
    "        start = int(idxs.min().item())\n",
    "        end_inclusive = int(idxs.max().item())\n",
    "        size_axis = mask.shape[axis]\n",
    "\n",
    "        start = max(0, start - self.margin)\n",
    "        end = min(size_axis, end_inclusive + 1 + self.margin)  # [start, end)\n",
    "\n",
    "        # Safety: never empty\n",
    "        if end <= start:\n",
    "            center = int((idxs.float().mean().round().item()))\n",
    "            start = max(0, min(center, size_axis - 1))\n",
    "            end = start + 1\n",
    "\n",
    "        return start, end\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "\n",
    "        if self.source_key not in d:\n",
    "            return d\n",
    "\n",
    "        crop_range = self._compute_crop_indices(d[self.source_key])\n",
    "        if crop_range is None:\n",
    "            return d  # nothing to crop\n",
    "\n",
    "        start, end = crop_range\n",
    "\n",
    "        def _safe_crop(arr):\n",
    "            arr_ndim = arr.ndim if hasattr(arr, \"ndim\") else np.asarray(arr).ndim\n",
    "            gaxis = self._get_spatial_axis_index(arr_ndim)\n",
    "            slicers = [slice(None)] * arr_ndim\n",
    "            slicers[gaxis] = slice(start, end)\n",
    "            out = arr[tuple(slicers)]\n",
    "            # --------- NEW: safety net, avoid 0-size dim ----------\n",
    "            if out.shape[gaxis] == 0:\n",
    "                return arr  # fallback to no crop for this key\n",
    "            # ------------------------------------------------------\n",
    "            return out\n",
    "\n",
    "        for key in self.keys:\n",
    "            if key not in d:\n",
    "                continue\n",
    "            d[key] = _safe_crop(d[key])\n",
    "\n",
    "            meta_key = f\"{key}_meta_dict\"\n",
    "            if meta_key in d and isinstance(d[meta_key], dict):\n",
    "                d[meta_key][\"spatial_shape\"] = np.asarray(\n",
    "                    d[key].shape[-3:], dtype=np.int64\n",
    "                )\n",
    "\n",
    "        # Also crop source_key itself if it's not already included\n",
    "        if self.source_key not in self.keys and self.source_key in d:\n",
    "            d[self.source_key] = _safe_crop(d[self.source_key])\n",
    "            meta_key = f\"{self.source_key}_meta_dict\"\n",
    "            if meta_key in d and isinstance(d[meta_key], dict):\n",
    "                d[meta_key][\"spatial_shape\"] = np.asarray(\n",
    "                    d[self.source_key].shape[-3:], dtype=np.int64\n",
    "                )\n",
    "\n",
    "        return d\n",
    "\n",
    "\n",
    "def remove_labels(x: torch.Tensor, labels: list, relabel: bool = False) -> torch.Tensor:\n",
    "    \"\"\"Remove the specified labels from the label tensor.\"\"\"\n",
    "    for label in labels:\n",
    "        x[x == label] = 0\n",
    "\n",
    "    if relabel:\n",
    "        # get unique values in tensor x\n",
    "        unique_values = x.unique()\n",
    "        # Sort the unique values\n",
    "        sorted_uv = sorted(unique_values)\n",
    "        # Remap the labels\n",
    "        for new_label, old_label in enumerate(sorted_uv):\n",
    "            x[x == old_label] = new_label\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acc5e868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-30 21:45:14,021 INFO image_writer.py:197 - writing: tmp/Patient_01799_Study_07874_Series_03.nii.gz\n",
      "2025-09-30 21:45:14,456 INFO image_writer.py:197 - writing: tmp/Patient_01799_Study_07874_Series_03_Body_filled.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "\n",
    "trans = monai.transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"mask\", \"body_filled_mask\"]),\n",
    "        # Get same orientation, spacing, and shape\n",
    "        transforms.EnsureChannelFirstd(keys=[\"mask\", \"body_filled_mask\"]),\n",
    "        transforms.Spacingd(\n",
    "            keys=[\"mask\", \"body_filled_mask\"],\n",
    "            pixdim=(1.0, 1.0, 2.0),\n",
    "            mode=(\"nearest\", \"nearest\"),\n",
    "        ),\n",
    "        transforms.Orientationd(keys=[\"mask\", \"body_filled_mask\"], axcodes=\"RAS\"),\n",
    "        # transforms.CropForegroundd(\n",
    "        #     keys=[\"mask\", \"body_filled_mask\"], source_key=\"mask\"\n",
    "        # ),\n",
    "        transforms.Lambdad(\n",
    "            keys=[\"mask\"],\n",
    "            func=functools.partial(\n",
    "                remove_labels, labels=[16, 17, 14]\n",
    "            ),  # Assuming labels 17 and 14 are the organs,\n",
    "        ),\n",
    "        CropForegroundAxisd(\n",
    "            keys=[\"mask\", \"body_filled_mask\"], source_key=\"mask\", axis=2\n",
    "        ),\n",
    "        transforms.CropForegroundd(keys=[\"mask\"], source_key=\"mask\"),\n",
    "        transforms.SaveImaged(\n",
    "            keys=[\"mask\", \"body_filled_mask\"],\n",
    "            output_dir=\"tmp\",\n",
    "            output_postfix=\"\",\n",
    "            separate_folder=False,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "transformed = trans(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c077abfa",
   "metadata": {},
   "source": [
    "## Dialation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b94f6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MONAI transform: grow non-largest components until connected, with debug snapshots\n",
    "from typing import Optional, Dict, Any, List\n",
    "import numpy as np\n",
    "import torch\n",
    "from monai.transforms import MapTransform\n",
    "from monai.config import KeysCollection\n",
    "from monai.utils import convert_to_numpy\n",
    "import cc3d\n",
    "from scipy.ndimage import binary_dilation, generate_binary_structure\n",
    "\n",
    "\n",
    "def _to_numpy(x):\n",
    "    if isinstance(x, torch.Tensor):\n",
    "        return x.detach().cpu().numpy()\n",
    "    return np.asarray(x)\n",
    "\n",
    "\n",
    "def _to_like_type(arr_np, like):\n",
    "    if isinstance(like, torch.Tensor):\n",
    "        return torch.from_numpy(arr_np).to(like.device, dtype=like.dtype)\n",
    "    return arr_np.astype(like.dtype, copy=False)\n",
    "\n",
    "\n",
    "def _ensure_binary(x, thr=0.5):\n",
    "    if x.dtype == np.bool_:\n",
    "        return x\n",
    "    if np.issubdtype(x.dtype, np.integer):\n",
    "        return x != 0\n",
    "    return x > thr\n",
    "\n",
    "\n",
    "class ConnectMinorByDilationRollbackd(MapTransform):\n",
    "    \"\"\"\n",
    "    Grow ONLY the non-largest components until they connect to the largest,\n",
    "    then (optionally) roll back dilation. Can export per-iteration snapshots.\n",
    "\n",
    "    New params vs earlier:\n",
    "      - cc_diagonal_ok: if True, use 26/8 connectivity for CC counting (diagonal touch counts as connected).\n",
    "      - dilate_diagonals: if True, dilation uses 26/8 neighborhood (grows diagonally too).\n",
    "      - debug: if True, write snapshots & logs to dict keys <debug_prefix>*.\n",
    "      - debug_every: keep every k-th dilation step (and the final one) to limit memory.\n",
    "      - debug_prefix: key prefix for debug outputs.\n",
    "      - alt_grow_anchor: if True and not connected after half max_iters, also dilate the largest\n",
    "                         every other iteration to help bridging wide gaps.\n",
    "      - if connection never achieved, we DO NOT roll back (you’ll get the last dilation).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        min_ratio: float = 0.05,\n",
    "        binarize_thr: float = 0.5,\n",
    "        max_iters: int = 512,\n",
    "        # connectivity controls\n",
    "        cc_diagonal_ok: bool = False,  # False => 6(3D)/4(2D) CC; True => 26/8 CC\n",
    "        dilate_diagonals: bool = False,  # False => face-only dilation; True => allow diagonal growth\n",
    "        # rollback\n",
    "        rollback_iters: int = 1,\n",
    "        rollback_mode: str = \"partial\",  # 'partial' | 'keep' | 'full'\n",
    "        # strategy\n",
    "        alt_grow_anchor: bool = False,\n",
    "        # debug\n",
    "        debug: bool = False,\n",
    "        debug_every: int = 1,\n",
    "        debug_prefix: str = \"post_debug\",\n",
    "        allow_missing_keys: bool = False,\n",
    "    ):\n",
    "        super().__init__(keys, allow_missing_keys)\n",
    "        self.min_ratio = float(min_ratio)\n",
    "        self.binarize_thr = float(binarize_thr)\n",
    "        self.max_iters = int(max_iters)\n",
    "        self.cc_diagonal_ok = bool(cc_diagonal_ok)\n",
    "        self.dilate_diagonals = bool(dilate_diagonals)\n",
    "        self.rollback_iters = int(max(0, rollback_iters))\n",
    "        assert rollback_mode in (\"partial\", \"keep\", \"full\")\n",
    "        self.rollback_mode = rollback_mode\n",
    "        self.alt_grow_anchor = bool(alt_grow_anchor)\n",
    "        self.debug = bool(debug)\n",
    "        self.debug_every = max(1, int(debug_every))\n",
    "        self.debug_prefix = str(debug_prefix)\n",
    "\n",
    "    # --- helpers ---\n",
    "    def _cc_rank(self, ndim):\n",
    "        # rank 1 ~ face-only (6/4), rank 3/2 ~ full (26/8)\n",
    "        if ndim == 2:\n",
    "            return 2 if self.cc_diagonal_ok else 1\n",
    "        return 3 if self.cc_diagonal_ok else 1\n",
    "\n",
    "    def _dilate_rank(self, ndim):\n",
    "        if ndim == 2:\n",
    "            return 2 if self.dilate_diagonals else 1\n",
    "        return 3 if self.dilate_diagonals else 1\n",
    "\n",
    "    def _label(self, mask_bool):\n",
    "        return cc3d.connected_components(mask_bool.astype(np.uint8), connectivity=6)\n",
    "\n",
    "    def _num_cc(self, mask_bool) -> int:\n",
    "        labels = self._label(mask_bool)\n",
    "        counts = np.bincount(labels.ravel())\n",
    "        if len(counts) == 0:\n",
    "            return 0\n",
    "        counts[0] = 0\n",
    "        return int((counts > 0).sum())\n",
    "\n",
    "    def _largest_and_others(self, mask_bool):\n",
    "        labels = self._label(mask_bool)\n",
    "        counts = np.bincount(labels.ravel())\n",
    "        counts[0] = 0\n",
    "        if (counts > 0).sum() == 0:\n",
    "            return np.zeros_like(mask_bool, bool), np.zeros_like(mask_bool, bool)\n",
    "        largest_id = int(np.argmax(counts))\n",
    "        L = labels == largest_id\n",
    "        O = mask_bool & (~L)\n",
    "        return L, O\n",
    "\n",
    "    def _prune_small(self, mask_bool):\n",
    "        labels = self._label(mask_bool)\n",
    "        counts = np.bincount(labels.ravel())\n",
    "        if len(counts) <= 1:\n",
    "            return mask_bool\n",
    "        counts[0] = 0\n",
    "        if (counts > 0).sum() <= 1:\n",
    "            return mask_bool\n",
    "        largest_id = int(np.argmax(counts))\n",
    "        largest_sz = int(counts[largest_id])\n",
    "        keep_ids = {\n",
    "            i\n",
    "            for i, c in enumerate(counts)\n",
    "            if (i != 0 and c >= self.min_ratio * largest_sz)\n",
    "        }\n",
    "        keep_ids.add(largest_id)\n",
    "        return np.isin(labels, list(keep_ids))\n",
    "\n",
    "    # --- core per-channel ---\n",
    "    def _process_one_channel(\n",
    "        self, mask_np: np.ndarray, dbg_store: Dict[str, Any]\n",
    "    ) -> np.ndarray:\n",
    "        bin_mask = _ensure_binary(mask_np, self.binarize_thr)\n",
    "\n",
    "        # nothing to do?\n",
    "        if self._num_cc(bin_mask) <= 1:\n",
    "            return mask_np\n",
    "\n",
    "        pruned = self._prune_small(bin_mask)\n",
    "        if self._num_cc(pruned) <= 1:\n",
    "            return pruned.astype(mask_np.dtype)\n",
    "\n",
    "        L, O0 = self._largest_and_others(pruned)\n",
    "        if not O0.any():\n",
    "            return pruned.astype(mask_np.dtype)\n",
    "\n",
    "        st = generate_binary_structure(pruned.ndim, self._dilate_rank(pruned.ndim))\n",
    "        O_hist: List[np.ndarray] = [O0]\n",
    "        L_hist: List[np.ndarray] = [L]  # stored only when alt_grow_anchor=True\n",
    "        cc_log = [self._num_cc(L | O0)]\n",
    "\n",
    "        O = O0.copy()\n",
    "        anchor = L.copy()\n",
    "        connected = False\n",
    "\n",
    "        for it in range(1, self.max_iters + 1):\n",
    "            # optionally alternate-grow the anchor to help crossing big gaps\n",
    "            if self.alt_grow_anchor and (it >= self.max_iters // 2) and (it % 2 == 0):\n",
    "                anchor = binary_dilation(anchor, structure=st)\n",
    "                L_hist.append(anchor)\n",
    "\n",
    "            O = binary_dilation(O, structure=st)\n",
    "            O_hist.append(O)\n",
    "\n",
    "            ncc = self._num_cc(anchor | O)\n",
    "            if (it % self.debug_every) == 0:\n",
    "                cc_log.append(ncc)\n",
    "\n",
    "            if ncc <= 1:\n",
    "                connected = True\n",
    "                break\n",
    "\n",
    "        # decide rollback\n",
    "        if not connected:\n",
    "            # no rollback if never connected: return last dilation so you can \"see the dilation\"\n",
    "            O_final = O_hist[-1]\n",
    "        else:\n",
    "            if self.rollback_mode == \"keep\":\n",
    "                O_final = O_hist[-1]\n",
    "            elif self.rollback_mode == \"full\":\n",
    "                O_final = O_hist[0]\n",
    "            else:  # partial\n",
    "                idx = max(0, len(O_hist) - 1 - self.rollback_iters)\n",
    "                O_final = O_hist[idx]\n",
    "\n",
    "        out_bool = anchor | O_final\n",
    "\n",
    "        # --- debug export ---\n",
    "        if self.debug:\n",
    "            # compact stacks: keep every k-th snapshot and always the last\n",
    "            sel = list(range(0, len(O_hist), self.debug_every))\n",
    "            if sel[-1] != len(O_hist) - 1:\n",
    "                sel.append(len(O_hist) - 1)\n",
    "            O_stack = np.stack([O_hist[i] for i in sel], axis=0).astype(\n",
    "                np.uint8\n",
    "            )  # [T,...]\n",
    "            dbg_store[\"O_stack\"] = O_stack  # others’ dilation snapshots\n",
    "            dbg_store[\"O_stack_iters\"] = np.array(sel, dtype=np.int32)\n",
    "            dbg_store[\"connected\"] = np.array([int(connected)], dtype=np.int32)\n",
    "            dbg_store[\"cc_log\"] = np.array(cc_log, dtype=np.int32)\n",
    "            if self.alt_grow_anchor:\n",
    "                # Anchor snapshots at the same cadence (best-effort)\n",
    "                L_sel = list(range(0, len(L_hist), self.debug_every))\n",
    "                if L_sel and L_sel[-1] != len(L_hist) - 1:\n",
    "                    L_sel.append(len(L_hist) - 1)\n",
    "                if L_hist:\n",
    "                    L_stack = np.stack(\n",
    "                        [L_hist[min(i, len(L_hist) - 1)] for i in L_sel], axis=0\n",
    "                    ).astype(np.uint8)\n",
    "                    dbg_store[\"L_stack\"] = L_stack\n",
    "                    dbg_store[\"L_stack_iters\"] = np.array(L_sel, dtype=np.int32)\n",
    "\n",
    "        return out_bool.astype(mask_np.dtype)\n",
    "\n",
    "    def _process_any(self, arr: np.ndarray, dbg_root: Dict[str, Any]):\n",
    "        # channel-first support\n",
    "        if arr.ndim >= 4 and arr.shape[0] > 1:\n",
    "            outs = []\n",
    "            for c in range(arr.shape[0]):\n",
    "                dbg_store = {}\n",
    "                outc = self._process_one_channel(arr[c], dbg_store)\n",
    "                if self.debug and dbg_store:\n",
    "                    dbg_root[f\"{self.debug_prefix}_ch{c}_O_stack\"] = dbg_store.get(\n",
    "                        \"O_stack\"\n",
    "                    )\n",
    "                    dbg_root[f\"{self.debug_prefix}_ch{c}_O_stack_iters\"] = (\n",
    "                        dbg_store.get(\"O_stack_iters\")\n",
    "                    )\n",
    "                    if \"L_stack\" in dbg_store:\n",
    "                        dbg_root[f\"{self.debug_prefix}_ch{c}_L_stack\"] = dbg_store.get(\n",
    "                            \"L_stack\"\n",
    "                        )\n",
    "                        dbg_root[f\"{self.debug_prefix}_ch{c}_L_stack_iters\"] = (\n",
    "                            dbg_store.get(\"L_stack_iters\")\n",
    "                        )\n",
    "                    dbg_root[f\"{self.debug_prefix}_ch{c}_connected\"] = dbg_store.get(\n",
    "                        \"connected\"\n",
    "                    )\n",
    "                    dbg_root[f\"{self.debug_prefix}_ch{c}_cc_log\"] = dbg_store.get(\n",
    "                        \"cc_log\"\n",
    "                    )\n",
    "                outs.append(outc)\n",
    "            return np.stack(outs, axis=0)\n",
    "        if arr.ndim >= 4 and arr.shape[0] == 1:\n",
    "            dbg_store = {}\n",
    "            out = self._process_one_channel(arr[0], dbg_store)\n",
    "            if self.debug and dbg_store:\n",
    "                dbg_root[f\"{self.debug_prefix}_ch0_O_stack\"] = dbg_store.get(\"O_stack\")\n",
    "                dbg_root[f\"{self.debug_prefix}_ch0_O_stack_iters\"] = dbg_store.get(\n",
    "                    \"O_stack_iters\"\n",
    "                )\n",
    "                if \"L_stack\" in dbg_store:\n",
    "                    dbg_root[f\"{self.debug_prefix}_ch0_L_stack\"] = dbg_store.get(\n",
    "                        \"L_stack\"\n",
    "                    )\n",
    "                    dbg_root[f\"{self.debug_prefix}_ch0_L_stack_iters\"] = dbg_store.get(\n",
    "                        \"L_stack_iters\"\n",
    "                    )\n",
    "                dbg_root[f\"{self.debug_prefix}_ch0_connected\"] = dbg_store.get(\n",
    "                    \"connected\"\n",
    "                )\n",
    "                dbg_root[f\"{self.debug_prefix}_ch0_cc_log\"] = dbg_store.get(\"cc_log\")\n",
    "            return out[None]\n",
    "        # no channel dim\n",
    "        dbg_store = {}\n",
    "        out = self._process_one_channel(arr, dbg_store)\n",
    "        if self.debug and dbg_store:\n",
    "            dbg_root[f\"{self.debug_prefix}_O_stack\"] = dbg_store.get(\"O_stack\")\n",
    "            dbg_root[f\"{self.debug_prefix}_O_stack_iters\"] = dbg_store.get(\n",
    "                \"O_stack_iters\"\n",
    "            )\n",
    "            if \"L_stack\" in dbg_store:\n",
    "                dbg_root[f\"{self.debug_prefix}_L_stack\"] = dbg_store.get(\"L_stack\")\n",
    "                dbg_root[f\"{self.debug_prefix}_L_stack_iters\"] = dbg_store.get(\n",
    "                    \"L_stack_iters\"\n",
    "                )\n",
    "            dbg_root[f\"{self.debug_prefix}_connected\"] = dbg_store.get(\"connected\")\n",
    "            dbg_root[f\"{self.debug_prefix}_cc_log\"] = dbg_store.get(\"cc_log\")\n",
    "        return out\n",
    "\n",
    "    # --- MONAI entrypoint ---\n",
    "    def __call__(self, data: Dict[str, Any]):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            if key not in d:\n",
    "                if not self.allow_missing_keys:\n",
    "                    raise KeyError(f\"Missing key: {key}\")\n",
    "                continue\n",
    "            arr = convert_to_numpy(d[key])\n",
    "            # store debug arrays back into the same dict\n",
    "            processed = self._process_any(_to_numpy(arr), d if self.debug else {})\n",
    "            d[key] = _to_like_type(processed, d[key])\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "077fe9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional, Union, Tuple\n",
    "from monai.transforms import Transform\n",
    "from monai.config import KeysCollection\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import (\n",
    "    binary_dilation,\n",
    "    binary_erosion,\n",
    "    label,\n",
    "    find_objects,\n",
    "    distance_transform_edt,\n",
    "    binary_closing,\n",
    ")\n",
    "import torch\n",
    "\n",
    "\n",
    "class ColonMaskPostProcessing(Transform):\n",
    "    \"\"\"\n",
    "    Post-processing transform for colon segmentation masks with natural appearance.\n",
    "\n",
    "    Steps:\n",
    "    1. Detect and handle thin unnatural connections (bottlenecks)\n",
    "    2. Identify connected components\n",
    "    3. Remove small components below volume ratio threshold\n",
    "    4. Join disjoint components using morphological operations\n",
    "    5. Smooth boundaries for natural appearance\n",
    "\n",
    "    Args:\n",
    "        volume_ratio_threshold: Minimum volume ratio compared to largest component (default: 0.1)\n",
    "        max_dilation_iterations: Maximum iterations for component joining (default: 10)\n",
    "        structuring_element_size: Size of structuring element for morphological ops (default: 3)\n",
    "        preserve_largest_only: If True, only keep largest component when single component expected (default: False)\n",
    "        min_neck_thickness: Minimum thickness for natural connections in voxels (default: 3)\n",
    "        smooth_iterations: Number of smoothing iterations for final result (default: 2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        volume_ratio_threshold: float = 0.1,\n",
    "        max_dilation_iterations: int = 10,\n",
    "        structuring_element_size: int = 3,\n",
    "        preserve_largest_only: bool = False,\n",
    "        min_neck_thickness: int = 3,\n",
    "        smooth_iterations: int = 2,\n",
    "    ):\n",
    "        self.volume_ratio_threshold = volume_ratio_threshold\n",
    "        self.max_dilation_iterations = max_dilation_iterations\n",
    "        self.structuring_element_size = structuring_element_size\n",
    "        self.preserve_largest_only = preserve_largest_only\n",
    "        self.min_neck_thickness = min_neck_thickness\n",
    "        self.smooth_iterations = smooth_iterations\n",
    "\n",
    "        # Create structuring element for morphological operations\n",
    "        self.struct_element = self._create_structuring_element()\n",
    "\n",
    "    def _create_structuring_element(self):\n",
    "        \"\"\"Create a spherical/ball structuring element\"\"\"\n",
    "        size = self.structuring_element_size\n",
    "        struct = ndimage.generate_binary_structure(3, 1)\n",
    "        struct = ndimage.iterate_structure(struct, size // 2)\n",
    "        return struct\n",
    "\n",
    "    def _detect_thin_connections(self, mask: np.ndarray) -> Tuple[np.ndarray, bool]:\n",
    "        \"\"\"\n",
    "        Detect and break thin unnatural connections (bottlenecks).\n",
    "        Uses distance transform to find regions where the mask is very thin.\n",
    "\n",
    "        Returns:\n",
    "            mask_cleaned: Mask with thin connections removed\n",
    "            has_thin_connections: Boolean indicating if thin connections were found\n",
    "        \"\"\"\n",
    "        # Compute distance transform (distance to nearest background voxel)\n",
    "        dist_transform = distance_transform_edt(mask)\n",
    "\n",
    "        # Find voxels that are part of thin structures\n",
    "        # These are voxels where the distance to background is very small\n",
    "        thin_voxels = (dist_transform > 0) & (\n",
    "            dist_transform < self.min_neck_thickness / 2\n",
    "        )\n",
    "\n",
    "        # Check if removing these thin voxels would disconnect the mask\n",
    "        test_mask = mask & ~thin_voxels\n",
    "        labeled_test, num_components_test = label(test_mask)\n",
    "\n",
    "        # Original connectivity check\n",
    "        labeled_original, num_components_original = label(mask)\n",
    "\n",
    "        # If removing thin voxels creates more components, we found thin connections\n",
    "        if num_components_test > num_components_original:\n",
    "            # These thin connections are likely artifacts, break them\n",
    "            return test_mask, True\n",
    "        else:\n",
    "            # No problematic thin connections, or they're essential to single component\n",
    "            return mask, False\n",
    "\n",
    "    def _get_connected_components(self, mask: np.ndarray):\n",
    "        \"\"\"\n",
    "        Identify and analyze connected components\n",
    "\n",
    "        Returns:\n",
    "            labeled_mask: Array with labeled components\n",
    "            component_sizes: Dictionary mapping label to size\n",
    "            num_components: Number of components\n",
    "        \"\"\"\n",
    "        labeled_mask, num_components = label(mask)\n",
    "\n",
    "        if num_components == 0:\n",
    "            return labeled_mask, {}, 0\n",
    "\n",
    "        # Calculate size of each component\n",
    "        component_sizes = {}\n",
    "        for i in range(1, num_components + 1):\n",
    "            component_sizes[i] = np.sum(labeled_mask == i)\n",
    "\n",
    "        return labeled_mask, component_sizes, num_components\n",
    "\n",
    "    def _remove_small_components(self, labeled_mask: np.ndarray, component_sizes: dict):\n",
    "        \"\"\"\n",
    "        Remove components below volume ratio threshold\n",
    "\n",
    "        Returns:\n",
    "            filtered_mask: Binary mask with small components removed\n",
    "            kept_labels: List of labels that were kept\n",
    "        \"\"\"\n",
    "        if not component_sizes:\n",
    "            return labeled_mask.astype(bool), []\n",
    "\n",
    "        # Find largest component size\n",
    "        max_size = max(component_sizes.values())\n",
    "        threshold_size = max_size * self.volume_ratio_threshold\n",
    "\n",
    "        # Keep only components above threshold\n",
    "        kept_labels = [\n",
    "            label for label, size in component_sizes.items() if size >= threshold_size\n",
    "        ]\n",
    "\n",
    "        # Create filtered mask\n",
    "        filtered_mask = np.zeros_like(labeled_mask, dtype=bool)\n",
    "        for kept_label in kept_labels:\n",
    "            filtered_mask |= labeled_mask == kept_label\n",
    "\n",
    "        return filtered_mask, kept_labels\n",
    "\n",
    "    def _smooth_boundaries(self, mask: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Smooth mask boundaries using morphological closing for natural appearance\n",
    "\n",
    "        Returns:\n",
    "            smoothed_mask: Mask with smoothed boundaries\n",
    "        \"\"\"\n",
    "        if self.smooth_iterations == 0:\n",
    "            return mask\n",
    "\n",
    "        smoothed = mask.copy()\n",
    "\n",
    "        # Create a smaller structuring element for gentle smoothing\n",
    "        smooth_struct = ndimage.generate_binary_structure(3, 1)\n",
    "\n",
    "        for _ in range(self.smooth_iterations):\n",
    "            # Morphological closing: dilation followed by erosion\n",
    "            # This fills small holes and smooths boundaries\n",
    "            smoothed = binary_closing(smoothed, structure=smooth_struct)\n",
    "\n",
    "        return smoothed\n",
    "\n",
    "    def _join_components_naturally(self, mask: np.ndarray):\n",
    "        \"\"\"\n",
    "        Join disjoint components using iterative dilation until connection,\n",
    "        then restore natural boundaries using controlled erosion and smoothing\n",
    "\n",
    "        Returns:\n",
    "            joined_mask: Binary mask with components joined naturally\n",
    "        \"\"\"\n",
    "        # Check if components are already connected\n",
    "        labeled_initial, num_initial = label(mask)\n",
    "        if num_initial <= 1:\n",
    "            return mask\n",
    "\n",
    "        # Store original mask for boundary restoration\n",
    "        original_mask = mask.copy()\n",
    "\n",
    "        # Get bounding boxes of components to estimate required dilation\n",
    "        component_props = find_objects(labeled_initial)\n",
    "\n",
    "        # Iteratively dilate until components join\n",
    "        dilated_mask = mask.copy()\n",
    "        iteration = 0\n",
    "        dilation_history = [mask.copy()]\n",
    "\n",
    "        while iteration < self.max_dilation_iterations:\n",
    "            # Dilate with structuring element\n",
    "            dilated_mask = binary_dilation(dilated_mask, structure=self.struct_element)\n",
    "            dilation_history.append(dilated_mask.copy())\n",
    "\n",
    "            # Check if components have joined\n",
    "            labeled_temp, num_components = label(dilated_mask)\n",
    "            if num_components == 1:\n",
    "                break\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        # If components didn't join after max iterations, return best attempt\n",
    "        if num_components > 1:\n",
    "            return dilated_mask\n",
    "\n",
    "        # Strategy for natural restoration:\n",
    "        # Use gradual erosion but stop before disconnection\n",
    "        restored_mask = dilated_mask.copy()\n",
    "\n",
    "        # Erode gradually, checking connectivity at each step\n",
    "        for i in range(iteration):\n",
    "            eroded_temp = binary_erosion(restored_mask, structure=self.struct_element)\n",
    "\n",
    "            # Check if erosion would disconnect components\n",
    "            labeled_check, num_check = label(eroded_temp)\n",
    "            if num_check > 1:\n",
    "                # Stop erosion to preserve connectivity\n",
    "                break\n",
    "\n",
    "            restored_mask = eroded_temp\n",
    "\n",
    "        # Alternative: Blend with original to preserve natural boundaries\n",
    "        # where possible while maintaining connections\n",
    "        bridge_region = dilated_mask & ~original_mask\n",
    "        natural_region = original_mask.copy()\n",
    "\n",
    "        # Combine: use original where it exists, bridges where needed\n",
    "        final_mask = natural_region | (restored_mask & bridge_region)\n",
    "\n",
    "        # Verify connectivity one more time\n",
    "        labeled_final, num_final = label(final_mask)\n",
    "        if num_final > 1:\n",
    "            # If still disconnected, use the restored mask\n",
    "            final_mask = restored_mask\n",
    "\n",
    "        return final_mask\n",
    "\n",
    "    def __call__(\n",
    "        self, mask: Union[np.ndarray, torch.Tensor]\n",
    "    ) -> Union[np.ndarray, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Apply post-processing to colon segmentation mask\n",
    "\n",
    "        Args:\n",
    "            mask: Binary segmentation mask (can be numpy array or torch tensor)\n",
    "\n",
    "        Returns:\n",
    "            Processed mask in same format as input\n",
    "        \"\"\"\n",
    "        # Handle torch tensors\n",
    "        is_torch = isinstance(mask, torch.Tensor)\n",
    "        if is_torch:\n",
    "            device = mask.device\n",
    "            dtype = mask.dtype\n",
    "            mask_np = mask.detach().cpu().numpy()\n",
    "        else:\n",
    "            mask_np = mask.copy()\n",
    "\n",
    "        # Ensure binary mask\n",
    "        mask_np = mask_np.astype(bool)\n",
    "\n",
    "        # Remove channel dimension if present\n",
    "        squeeze_dim = False\n",
    "        if mask_np.ndim == 4 and mask_np.shape[0] == 1:\n",
    "            mask_np = mask_np[0]\n",
    "            squeeze_dim = True\n",
    "\n",
    "        # Step 0: Detect and handle thin unnatural connections\n",
    "        mask_np, had_thin_connections = self._detect_thin_connections(mask_np)\n",
    "\n",
    "        # Step 1: Get connected components\n",
    "        labeled_mask, component_sizes, num_components = self._get_connected_components(\n",
    "            mask_np\n",
    "        )\n",
    "\n",
    "        # If no components, return empty mask\n",
    "        if num_components == 0:\n",
    "            result = mask_np\n",
    "\n",
    "        # If single component, just smooth it\n",
    "        elif num_components == 1:\n",
    "            result = self._smooth_boundaries(mask_np)\n",
    "\n",
    "        # Multiple components: process\n",
    "        else:\n",
    "            # Step 2: Remove small components\n",
    "            filtered_mask, kept_labels = self._remove_small_components(\n",
    "                labeled_mask, component_sizes\n",
    "            )\n",
    "\n",
    "            # If only one component remains after filtering, smooth and return\n",
    "            filtered_labeled, filtered_num = label(filtered_mask)\n",
    "            if filtered_num <= 1:\n",
    "                result = self._smooth_boundaries(filtered_mask)\n",
    "            else:\n",
    "                # Step 3: Join remaining components naturally\n",
    "                joined_mask = self._join_components_naturally(filtered_mask)\n",
    "\n",
    "                # Step 4: Final smoothing for natural appearance\n",
    "                result = self._smooth_boundaries(joined_mask)\n",
    "\n",
    "        # Restore dimensions\n",
    "        if squeeze_dim:\n",
    "            result = result[np.newaxis, ...]\n",
    "\n",
    "        # Convert back to torch if needed\n",
    "        if is_torch:\n",
    "            result = torch.from_numpy(result.astype(np.float32)).to(\n",
    "                device=device, dtype=dtype\n",
    "            )\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class ColonMaskPostProcessingd(Transform):\n",
    "    \"\"\"\n",
    "    Dictionary-based version for use with MONAI dictionary transforms\n",
    "\n",
    "    Args:\n",
    "        keys: Keys to apply the transform to\n",
    "        volume_ratio_threshold: Minimum volume ratio compared to largest component\n",
    "        max_dilation_iterations: Maximum iterations for component joining\n",
    "        structuring_element_size: Size of structuring element\n",
    "        preserve_largest_only: If True, only keep largest component\n",
    "        min_neck_thickness: Minimum thickness for natural connections in voxels\n",
    "        smooth_iterations: Number of smoothing iterations for final result\n",
    "        allow_missing_keys: If True, don't raise error for missing keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        volume_ratio_threshold: float = 0.1,\n",
    "        max_dilation_iterations: int = 10,\n",
    "        structuring_element_size: int = 3,\n",
    "        preserve_largest_only: bool = False,\n",
    "        min_neck_thickness: int = 4,\n",
    "        smooth_iterations: int = 10,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ):\n",
    "        self.keys = keys if isinstance(keys, (list, tuple)) else [keys]\n",
    "        self.transform = ColonMaskPostProcessing(\n",
    "            volume_ratio_threshold=volume_ratio_threshold,\n",
    "            max_dilation_iterations=max_dilation_iterations,\n",
    "            structuring_element_size=structuring_element_size,\n",
    "            preserve_largest_only=preserve_largest_only,\n",
    "            min_neck_thickness=min_neck_thickness,\n",
    "            smooth_iterations=smooth_iterations,\n",
    "        )\n",
    "        self.allow_missing_keys = allow_missing_keys\n",
    "\n",
    "    def __call__(self, data: dict) -> dict:\n",
    "        \"\"\"Apply transform to dictionary data\"\"\"\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            if key in d:\n",
    "                d[key] = self.transform(d[key])\n",
    "            elif not self.allow_missing_keys:\n",
    "                raise KeyError(f\"Key '{key}' not found in data dictionary\")\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1a6105ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "from monai.transforms import Transform\n",
    "from monai.config import KeysCollection\n",
    "from scipy.ndimage import binary_dilation, binary_erosion, generate_binary_structure\n",
    "import torch\n",
    "\n",
    "\n",
    "class SmoothColonMask(Transform):\n",
    "    \"\"\"\n",
    "    Simple smoothing transform for colon segmentation masks.\n",
    "\n",
    "    How it works:\n",
    "    1. DILATE: Expands the mask outward (fills gaps, connects close parts)\n",
    "    2. ERODE: Shrinks back by same amount (returns to original size but smoothed)\n",
    "\n",
    "    This removes weird appendages and smooths irregular boundaries.\n",
    "\n",
    "    Args:\n",
    "        iterations: Number of dilation/erosion iterations (default: 3)\n",
    "                   Higher = more aggressive smoothing\n",
    "                   Typical range: 2-5\n",
    "        connectivity: Structuring element connectivity (1, 2, or 3)\n",
    "                     1 = face connectivity (6-connected in 3D)\n",
    "                     2 = face+edge connectivity (18-connected in 3D)\n",
    "                     3 = face+edge+corner connectivity (26-connected in 3D)\n",
    "                     Default: 2 (good balance)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, iterations: int = 3, connectivity: int = 2):\n",
    "        self.iterations = iterations\n",
    "        self.connectivity = connectivity\n",
    "\n",
    "        # Create structuring element (defines the shape of dilation/erosion)\n",
    "        # This is a 3D ball-like structure\n",
    "        self.struct_element = generate_binary_structure(3, connectivity)\n",
    "\n",
    "    def __call__(\n",
    "        self, mask: Union[np.ndarray, torch.Tensor]\n",
    "    ) -> Union[np.ndarray, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Apply smoothing to mask\n",
    "\n",
    "        Args:\n",
    "            mask: Binary segmentation mask\n",
    "\n",
    "        Returns:\n",
    "            Smoothed mask in same format as input\n",
    "        \"\"\"\n",
    "        # Handle torch tensors\n",
    "        is_torch = isinstance(mask, torch.Tensor)\n",
    "        if is_torch:\n",
    "            device = mask.device\n",
    "            dtype = mask.dtype\n",
    "            mask_np = mask.detach().cpu().numpy()\n",
    "        else:\n",
    "            mask_np = mask.copy()\n",
    "\n",
    "        # Ensure binary\n",
    "        mask_np = mask_np.astype(bool)\n",
    "\n",
    "        # Handle channel dimension\n",
    "        squeeze_dim = False\n",
    "        if mask_np.ndim == 4 and mask_np.shape[0] == 1:\n",
    "            mask_np = mask_np[0]\n",
    "            squeeze_dim = True\n",
    "\n",
    "        # STEP 1: Dilate (expand outward) - fills gaps, smooths bumps outward\n",
    "        dilated = mask_np.copy()\n",
    "        for _ in range(self.iterations):\n",
    "            dilated = binary_dilation(dilated, structure=self.struct_element)\n",
    "\n",
    "        # STEP 2: Erode (shrink back) - returns to approximately original size\n",
    "        smoothed = dilated.copy()\n",
    "        for _ in range(self.iterations):\n",
    "            smoothed = binary_erosion(smoothed, structure=self.struct_element)\n",
    "\n",
    "        # Restore dimensions\n",
    "        if squeeze_dim:\n",
    "            smoothed = smoothed[np.newaxis, ...]\n",
    "\n",
    "        # Convert back to torch if needed\n",
    "        if is_torch:\n",
    "            smoothed = torch.from_numpy(smoothed.astype(np.float32)).to(\n",
    "                device=device, dtype=dtype\n",
    "            )\n",
    "\n",
    "        return smoothed\n",
    "\n",
    "\n",
    "class SmoothColonMaskd(Transform):\n",
    "    \"\"\"\n",
    "    Dictionary-based version for MONAI pipelines\n",
    "\n",
    "    Args:\n",
    "        keys: Keys to apply transform to\n",
    "        iterations: Number of dilation/erosion iterations (default: 3)\n",
    "        connectivity: Structuring element connectivity (1, 2, or 3)\n",
    "        allow_missing_keys: Don't raise error for missing keys\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys: KeysCollection,\n",
    "        iterations: int = 3,\n",
    "        connectivity: int = 2,\n",
    "        allow_missing_keys: bool = False,\n",
    "    ):\n",
    "        self.keys = keys if isinstance(keys, (list, tuple)) else [keys]\n",
    "        self.transform = SmoothColonMask(\n",
    "            iterations=iterations, connectivity=connectivity\n",
    "        )\n",
    "        self.allow_missing_keys = allow_missing_keys\n",
    "\n",
    "    def __call__(self, data: dict) -> dict:\n",
    "        \"\"\"Apply transform to dictionary\"\"\"\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            if key in d:\n",
    "                d[key] = self.transform(d[key])\n",
    "            elif not self.allow_missing_keys:\n",
    "                raise KeyError(f\"Key '{key}' not found in data\")\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30be8fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-06 23:14:59,280 INFO image_writer.py:197 - writing: tmp/0.nii.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'img': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]])}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import monai\n",
    "import json, os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from monai import transforms\n",
    "\n",
    "nii_gz_file = {\n",
    "    \"img\": \"/home/yb107/cvpr2025/DukeDiffSeg/outputs/diffunet-binary-colon/5.1/inference_c_grade_550_gs_3.0/Patient_00135_Study_03256_Series_03_pred.nii.gz\"\n",
    "}\n",
    "transforms = monai.transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=\"img\", image_only=True),\n",
    "        transforms.EnsureChannelFirstd(keys=\"img\"),\n",
    "        # ColonMaskPostProcessingd(\n",
    "        #     keys=\"img\",\n",
    "        # ),\n",
    "        SmoothColonMaskd(keys=\"img\", iterations=6, connectivity=2),\n",
    "        transforms.SaveImaged(\n",
    "            keys=\"img\", output_dir=\"tmp\", output_postfix=\"\", separate_folder=False\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "transforms(nii_gz_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DukeDiffSeg-HooVw7aP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
